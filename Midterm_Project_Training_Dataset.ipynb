{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongSeong/brainML_HW/blob/main/Midterm_Project_Training_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wStZFhkP0oiW"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL-WIKzpphlK"
      },
      "source": [
        "**Load TADPOLE* dataset (csv file) from Google Drive**\n",
        "-------------------------------------------------------\n",
        "*The Alzheimer's Disease Prediction Of Longitudinal Evolution\n",
        "(https://tadpole.grand-challenge.org/)\n",
        "\n",
        "### -Subjects: 1707 (1363 Train (80%) + 344 Test (20%))\n",
        "\n",
        "\n",
        "### -Features: 72\n",
        "*   2 demographic feature: MMSE, ADAS13\n",
        "*   70 mean values of cortical thickness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS_uk0HaweJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "746f1164-d3a8-4275-d55b-74a9bc951d0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "csv_file_train = '/content/gdrive/My Drive/BNCS401_Midterm_Project/Train_data_reupdated.csv'  # Set your path\n",
        "train_data = pd.read_csv(csv_file_train)\n",
        "train_data\n",
        "\n",
        "# DXCHANGE: clinical label (1-CN, 2-MCI, 3-AD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RID</th>\n",
              "      <th>DXCHANGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>MMSE</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4084</td>\n",
              "      <td>1</td>\n",
              "      <td>68.4</td>\n",
              "      <td>30</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2.700</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.613</td>\n",
              "      <td>2.904</td>\n",
              "      <td>2.311</td>\n",
              "      <td>1.647</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.652</td>\n",
              "      <td>2.604</td>\n",
              "      <td>2.480</td>\n",
              "      <td>3.095</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.207</td>\n",
              "      <td>2.903</td>\n",
              "      <td>2.617</td>\n",
              "      <td>4.117</td>\n",
              "      <td>2.701</td>\n",
              "      <td></td>\n",
              "      <td>3.127</td>\n",
              "      <td>3.051</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.872</td>\n",
              "      <td>2.732</td>\n",
              "      <td>2.026</td>\n",
              "      <td>3.756</td>\n",
              "      <td>2.813</td>\n",
              "      <td>2.762</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.695</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.690</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.421</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.370</td>\n",
              "      <td>2.674</td>\n",
              "      <td>3.004</td>\n",
              "      <td>2.369</td>\n",
              "      <td>1.599</td>\n",
              "      <td>2.208</td>\n",
              "      <td>2.650</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.544</td>\n",
              "      <td>3.018</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.880</td>\n",
              "      <td>2.322</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.489</td>\n",
              "      <td>3.620</td>\n",
              "      <td>2.711</td>\n",
              "      <td></td>\n",
              "      <td>2.593</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.660</td>\n",
              "      <td>1.993</td>\n",
              "      <td>3.734</td>\n",
              "      <td>2.390</td>\n",
              "      <td>2.817</td>\n",
              "      <td>2.471</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.667</td>\n",
              "      <td>2.490</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.171</td>\n",
              "      <td>2.862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2196</td>\n",
              "      <td>2</td>\n",
              "      <td>68.2</td>\n",
              "      <td>30</td>\n",
              "      <td>13.00</td>\n",
              "      <td>2.453</td>\n",
              "      <td>2.992</td>\n",
              "      <td>2.470</td>\n",
              "      <td>2.965</td>\n",
              "      <td>2.438</td>\n",
              "      <td>1.584</td>\n",
              "      <td>1.910</td>\n",
              "      <td>2.900</td>\n",
              "      <td>2.451</td>\n",
              "      <td>2.335</td>\n",
              "      <td>2.771</td>\n",
              "      <td>2.354</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.363</td>\n",
              "      <td>3.613</td>\n",
              "      <td>2.475</td>\n",
              "      <td></td>\n",
              "      <td>3.196</td>\n",
              "      <td>3.334</td>\n",
              "      <td>2.343</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.627</td>\n",
              "      <td>1.742</td>\n",
              "      <td>3.383</td>\n",
              "      <td>2.647</td>\n",
              "      <td>2.758</td>\n",
              "      <td>2.394</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.241</td>\n",
              "      <td>2.824</td>\n",
              "      <td>1.865</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.866</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.874</td>\n",
              "      <td>2.316</td>\n",
              "      <td>1.478</td>\n",
              "      <td>1.909</td>\n",
              "      <td>2.780</td>\n",
              "      <td>2.589</td>\n",
              "      <td>2.133</td>\n",
              "      <td>3.036</td>\n",
              "      <td>2.329</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.070</td>\n",
              "      <td>2.783</td>\n",
              "      <td>2.594</td>\n",
              "      <td>3.405</td>\n",
              "      <td>2.367</td>\n",
              "      <td></td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.977</td>\n",
              "      <td>2.489</td>\n",
              "      <td>1.868</td>\n",
              "      <td>3.220</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.372</td>\n",
              "      <td>2.854</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.233</td>\n",
              "      <td>2.793</td>\n",
              "      <td>1.987</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>657</td>\n",
              "      <td>1</td>\n",
              "      <td>77.7</td>\n",
              "      <td>29</td>\n",
              "      <td>15.33</td>\n",
              "      <td>2.249</td>\n",
              "      <td>2.296</td>\n",
              "      <td>2.315</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.420</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.327</td>\n",
              "      <td>2.193</td>\n",
              "      <td>2.415</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.559</td>\n",
              "      <td>2.008</td>\n",
              "      <td>2.495</td>\n",
              "      <td>2.418</td>\n",
              "      <td>4.210</td>\n",
              "      <td>2.167</td>\n",
              "      <td>1.08</td>\n",
              "      <td>3.362</td>\n",
              "      <td>3.077</td>\n",
              "      <td>2.648</td>\n",
              "      <td>2.759</td>\n",
              "      <td>2.442</td>\n",
              "      <td>1.717</td>\n",
              "      <td>2.886</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.737</td>\n",
              "      <td>2.402</td>\n",
              "      <td>3.012</td>\n",
              "      <td>2.418</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.805</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.318</td>\n",
              "      <td>3.025</td>\n",
              "      <td>2.270</td>\n",
              "      <td>1.831</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.908</td>\n",
              "      <td>2.335</td>\n",
              "      <td>1.389</td>\n",
              "      <td>1.765</td>\n",
              "      <td>2.397</td>\n",
              "      <td>2.187</td>\n",
              "      <td>2.188</td>\n",
              "      <td>2.889</td>\n",
              "      <td>2.318</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.028</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.309</td>\n",
              "      <td>3.880</td>\n",
              "      <td>1.832</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.343</td>\n",
              "      <td>2.363</td>\n",
              "      <td>1.601</td>\n",
              "      <td>3.683</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.385</td>\n",
              "      <td>2.365</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.415</td>\n",
              "      <td>2.252</td>\n",
              "      <td>2.583</td>\n",
              "      <td>1.850</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4526</td>\n",
              "      <td>3</td>\n",
              "      <td>79.4</td>\n",
              "      <td>22</td>\n",
              "      <td>24.00</td>\n",
              "      <td>2.197</td>\n",
              "      <td>2.289</td>\n",
              "      <td>2.258</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.124</td>\n",
              "      <td>1.473</td>\n",
              "      <td>1.729</td>\n",
              "      <td>2.680</td>\n",
              "      <td>2.184</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.680</td>\n",
              "      <td>2.218</td>\n",
              "      <td>2.407</td>\n",
              "      <td>1.923</td>\n",
              "      <td>2.542</td>\n",
              "      <td>2.296</td>\n",
              "      <td>3.135</td>\n",
              "      <td>2.157</td>\n",
              "      <td></td>\n",
              "      <td>2.671</td>\n",
              "      <td>2.925</td>\n",
              "      <td>2.288</td>\n",
              "      <td>3.322</td>\n",
              "      <td>2.336</td>\n",
              "      <td>1.691</td>\n",
              "      <td>2.399</td>\n",
              "      <td>2.940</td>\n",
              "      <td>2.178</td>\n",
              "      <td>2.079</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.083</td>\n",
              "      <td>1.904</td>\n",
              "      <td>2.400</td>\n",
              "      <td>1.750</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.493</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.244</td>\n",
              "      <td>2.399</td>\n",
              "      <td>2.308</td>\n",
              "      <td>2.073</td>\n",
              "      <td>1.506</td>\n",
              "      <td>1.777</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.319</td>\n",
              "      <td>2.087</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.410</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.269</td>\n",
              "      <td>3.085</td>\n",
              "      <td>2.080</td>\n",
              "      <td></td>\n",
              "      <td>2.503</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.372</td>\n",
              "      <td>1.757</td>\n",
              "      <td>2.651</td>\n",
              "      <td>2.665</td>\n",
              "      <td>2.578</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.537</td>\n",
              "      <td>1.958</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>362</td>\n",
              "      <td>2</td>\n",
              "      <td>70.5</td>\n",
              "      <td>24</td>\n",
              "      <td>20.33</td>\n",
              "      <td>1.765</td>\n",
              "      <td>2.081</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.461</td>\n",
              "      <td>2.140</td>\n",
              "      <td>1.466</td>\n",
              "      <td>1.749</td>\n",
              "      <td>2.025</td>\n",
              "      <td>2.021</td>\n",
              "      <td>1.938</td>\n",
              "      <td>2.266</td>\n",
              "      <td>2.114</td>\n",
              "      <td>2.368</td>\n",
              "      <td>1.811</td>\n",
              "      <td>2.227</td>\n",
              "      <td>2.191</td>\n",
              "      <td>3.188</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.013</td>\n",
              "      <td>2.548</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.280</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.291</td>\n",
              "      <td>1.821</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.248</td>\n",
              "      <td>2.414</td>\n",
              "      <td>2.164</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.012</td>\n",
              "      <td>1.916</td>\n",
              "      <td>2.451</td>\n",
              "      <td>1.959</td>\n",
              "      <td>2.519</td>\n",
              "      <td>2.405</td>\n",
              "      <td>1.968</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.201</td>\n",
              "      <td>2.711</td>\n",
              "      <td>2.287</td>\n",
              "      <td>1.753</td>\n",
              "      <td>1.644</td>\n",
              "      <td>2.209</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.745</td>\n",
              "      <td>2.102</td>\n",
              "      <td>2.519</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.099</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.191</td>\n",
              "      <td>0.922</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.273</td>\n",
              "      <td>1.662</td>\n",
              "      <td>2.790</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.348</td>\n",
              "      <td>2.197</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.844</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.121</td>\n",
              "      <td>1.951</td>\n",
              "      <td>2.262</td>\n",
              "      <td>2.245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>4187</td>\n",
              "      <td>2</td>\n",
              "      <td>62.0</td>\n",
              "      <td>29</td>\n",
              "      <td>17.00</td>\n",
              "      <td>2.441</td>\n",
              "      <td>2.813</td>\n",
              "      <td>2.614</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.354</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.395</td>\n",
              "      <td>2.923</td>\n",
              "      <td>2.005</td>\n",
              "      <td>2.571</td>\n",
              "      <td>2.312</td>\n",
              "      <td>2.923</td>\n",
              "      <td>2.596</td>\n",
              "      <td>3.972</td>\n",
              "      <td>2.793</td>\n",
              "      <td></td>\n",
              "      <td>3.396</td>\n",
              "      <td>3.230</td>\n",
              "      <td>2.417</td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.544</td>\n",
              "      <td>1.938</td>\n",
              "      <td>3.513</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.371</td>\n",
              "      <td>2.907</td>\n",
              "      <td>2.434</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.717</td>\n",
              "      <td>2.096</td>\n",
              "      <td>2.335</td>\n",
              "      <td>2.996</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.851</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.426</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.476</td>\n",
              "      <td>2.625</td>\n",
              "      <td>2.357</td>\n",
              "      <td>3.116</td>\n",
              "      <td>2.263</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.533</td>\n",
              "      <td>3.715</td>\n",
              "      <td>2.815</td>\n",
              "      <td></td>\n",
              "      <td>2.464</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.035</td>\n",
              "      <td>3.865</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.850</td>\n",
              "      <td>2.524</td>\n",
              "      <td>3.213</td>\n",
              "      <td>2.489</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.209</td>\n",
              "      <td>2.310</td>\n",
              "      <td>2.935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>4928</td>\n",
              "      <td>2</td>\n",
              "      <td>77.8</td>\n",
              "      <td>27</td>\n",
              "      <td>17.00</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.246</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.392</td>\n",
              "      <td>2.158</td>\n",
              "      <td>1.686</td>\n",
              "      <td>1.914</td>\n",
              "      <td>2.318</td>\n",
              "      <td>2.297</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.908</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.067</td>\n",
              "      <td>2.385</td>\n",
              "      <td>2.458</td>\n",
              "      <td>2.749</td>\n",
              "      <td>2.360</td>\n",
              "      <td></td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.919</td>\n",
              "      <td>2.063</td>\n",
              "      <td>2.426</td>\n",
              "      <td>2.284</td>\n",
              "      <td>1.906</td>\n",
              "      <td>3.070</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.235</td>\n",
              "      <td>2.685</td>\n",
              "      <td>2.419</td>\n",
              "      <td>1.953</td>\n",
              "      <td>2.660</td>\n",
              "      <td>1.815</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.349</td>\n",
              "      <td>2.211</td>\n",
              "      <td>2.330</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.196</td>\n",
              "      <td>1.565</td>\n",
              "      <td>1.845</td>\n",
              "      <td>2.281</td>\n",
              "      <td>2.255</td>\n",
              "      <td>2.266</td>\n",
              "      <td>2.893</td>\n",
              "      <td>2.167</td>\n",
              "      <td>2.420</td>\n",
              "      <td>1.934</td>\n",
              "      <td>2.468</td>\n",
              "      <td>2.420</td>\n",
              "      <td>3.288</td>\n",
              "      <td>2.411</td>\n",
              "      <td></td>\n",
              "      <td>2.147</td>\n",
              "      <td>2.478</td>\n",
              "      <td>2.319</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.106</td>\n",
              "      <td>2.742</td>\n",
              "      <td>2.369</td>\n",
              "      <td>2.221</td>\n",
              "      <td>2.485</td>\n",
              "      <td>2.162</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.375</td>\n",
              "      <td>1.917</td>\n",
              "      <td>2.199</td>\n",
              "      <td>2.709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>4887</td>\n",
              "      <td>3</td>\n",
              "      <td>73.8</td>\n",
              "      <td>26</td>\n",
              "      <td>27.00</td>\n",
              "      <td>1.996</td>\n",
              "      <td>2.545</td>\n",
              "      <td>2.196</td>\n",
              "      <td>2.263</td>\n",
              "      <td>2.099</td>\n",
              "      <td>1.343</td>\n",
              "      <td>1.858</td>\n",
              "      <td>2.676</td>\n",
              "      <td>2.135</td>\n",
              "      <td>1.955</td>\n",
              "      <td>2.882</td>\n",
              "      <td>2.042</td>\n",
              "      <td>2.448</td>\n",
              "      <td>1.874</td>\n",
              "      <td>2.486</td>\n",
              "      <td>2.061</td>\n",
              "      <td>3.193</td>\n",
              "      <td>1.800</td>\n",
              "      <td></td>\n",
              "      <td>3.054</td>\n",
              "      <td>2.921</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.629</td>\n",
              "      <td>2.119</td>\n",
              "      <td>1.730</td>\n",
              "      <td>3.190</td>\n",
              "      <td>2.839</td>\n",
              "      <td>2.443</td>\n",
              "      <td>1.948</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.567</td>\n",
              "      <td>1.854</td>\n",
              "      <td>2.713</td>\n",
              "      <td>1.646</td>\n",
              "      <td>2.698</td>\n",
              "      <td>2.424</td>\n",
              "      <td>1.969</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.231</td>\n",
              "      <td>2.447</td>\n",
              "      <td>2.340</td>\n",
              "      <td>1.472</td>\n",
              "      <td>1.775</td>\n",
              "      <td>2.832</td>\n",
              "      <td>2.014</td>\n",
              "      <td>2.200</td>\n",
              "      <td>2.720</td>\n",
              "      <td>2.320</td>\n",
              "      <td>2.427</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.362</td>\n",
              "      <td>2.122</td>\n",
              "      <td>3.244</td>\n",
              "      <td>2.270</td>\n",
              "      <td></td>\n",
              "      <td>2.577</td>\n",
              "      <td>2.239</td>\n",
              "      <td>2.084</td>\n",
              "      <td>1.682</td>\n",
              "      <td>2.797</td>\n",
              "      <td>3.351</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.585</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.683</td>\n",
              "      <td>1.701</td>\n",
              "      <td>2.749</td>\n",
              "      <td>2.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>1205</td>\n",
              "      <td>3</td>\n",
              "      <td>83.0</td>\n",
              "      <td>23</td>\n",
              "      <td>29.33</td>\n",
              "      <td>1.822</td>\n",
              "      <td>2.508</td>\n",
              "      <td>2.174</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.000</td>\n",
              "      <td>1.374</td>\n",
              "      <td>1.465</td>\n",
              "      <td>2.589</td>\n",
              "      <td>1.677</td>\n",
              "      <td>1.932</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.215</td>\n",
              "      <td>1.617</td>\n",
              "      <td>2.066</td>\n",
              "      <td>1.910</td>\n",
              "      <td>3.069</td>\n",
              "      <td>1.536</td>\n",
              "      <td>1.043</td>\n",
              "      <td>2.902</td>\n",
              "      <td>2.809</td>\n",
              "      <td>1.875</td>\n",
              "      <td>2.716</td>\n",
              "      <td>2.001</td>\n",
              "      <td>1.396</td>\n",
              "      <td>2.681</td>\n",
              "      <td>1.613</td>\n",
              "      <td>2.239</td>\n",
              "      <td>1.966</td>\n",
              "      <td>2.561</td>\n",
              "      <td>2.520</td>\n",
              "      <td>1.648</td>\n",
              "      <td>2.414</td>\n",
              "      <td>1.736</td>\n",
              "      <td>1.962</td>\n",
              "      <td>2.296</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.468</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.079</td>\n",
              "      <td>1.926</td>\n",
              "      <td>1.232</td>\n",
              "      <td>1.501</td>\n",
              "      <td>2.476</td>\n",
              "      <td>1.831</td>\n",
              "      <td>1.939</td>\n",
              "      <td>3.250</td>\n",
              "      <td>1.900</td>\n",
              "      <td>2.403</td>\n",
              "      <td>1.680</td>\n",
              "      <td>2.193</td>\n",
              "      <td>1.968</td>\n",
              "      <td>3.351</td>\n",
              "      <td>1.494</td>\n",
              "      <td>0.997</td>\n",
              "      <td>1.828</td>\n",
              "      <td>3.025</td>\n",
              "      <td>1.852</td>\n",
              "      <td>1.463</td>\n",
              "      <td>2.982</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.408</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.449</td>\n",
              "      <td>2.574</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.515</td>\n",
              "      <td>1.806</td>\n",
              "      <td>2.071</td>\n",
              "      <td>2.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>2099</td>\n",
              "      <td>2</td>\n",
              "      <td>83.7</td>\n",
              "      <td>30</td>\n",
              "      <td>12.00</td>\n",
              "      <td>2.245</td>\n",
              "      <td>2.351</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.475</td>\n",
              "      <td>2.155</td>\n",
              "      <td>1.676</td>\n",
              "      <td>1.857</td>\n",
              "      <td>2.525</td>\n",
              "      <td>2.085</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.161</td>\n",
              "      <td>2.295</td>\n",
              "      <td>1.948</td>\n",
              "      <td>2.424</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.958</td>\n",
              "      <td>1.778</td>\n",
              "      <td></td>\n",
              "      <td>2.672</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.693</td>\n",
              "      <td>2.315</td>\n",
              "      <td>1.637</td>\n",
              "      <td>3.445</td>\n",
              "      <td>2.396</td>\n",
              "      <td>2.432</td>\n",
              "      <td>2.265</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.021</td>\n",
              "      <td>2.350</td>\n",
              "      <td>1.699</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.625</td>\n",
              "      <td>2.034</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.237</td>\n",
              "      <td>2.236</td>\n",
              "      <td>2.314</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.868</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.072</td>\n",
              "      <td>2.060</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.155</td>\n",
              "      <td>2.376</td>\n",
              "      <td>1.894</td>\n",
              "      <td>2.400</td>\n",
              "      <td>2.330</td>\n",
              "      <td>3.177</td>\n",
              "      <td>2.054</td>\n",
              "      <td></td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.433</td>\n",
              "      <td>2.206</td>\n",
              "      <td>1.785</td>\n",
              "      <td>3.093</td>\n",
              "      <td>2.310</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.381</td>\n",
              "      <td>1.817</td>\n",
              "      <td>2.121</td>\n",
              "      <td>2.586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1363 rows × 75 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       RID  ...  ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
              "0     4084  ...                                       2.862\n",
              "1     2196  ...                                       2.943\n",
              "2      657  ...                                       2.828\n",
              "3     4526  ...                                       2.591\n",
              "4      362  ...                                       2.245\n",
              "...    ...  ...                                         ...\n",
              "1358  4187  ...                                       2.935\n",
              "1359  4928  ...                                       2.709\n",
              "1360  4887  ...                                       2.777\n",
              "1361  1205  ...                                       2.248\n",
              "1362  2099  ...                                       2.586\n",
              "\n",
              "[1363 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3aPdHTByT8B"
      },
      "source": [
        "#WRITE YOUR CODE HERE!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuFRiRUyAFL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "1e619a8f-2c79-407b-ed05-b67a24b12b00"
      },
      "source": [
        "# Build your model\n",
        "for i in train_data.columns:#since there are object type data and null, it needs to be changed.\n",
        "  if train_data[i].dtype=='object':#for object type data column.\n",
        "    train_data[i]=pd.to_numeric(train_data[i], errors='coerce')#numeric entries are changed to float type and missing entries are replaced with NaN.\n",
        "  if train_data[i].isnull().any():#missing entries are replaced with NaN.\n",
        "    train_data[i].loc[train_data[i].isnull()]=np.NaN\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "train_data= SimpleImputer(strategy='median').fit_transform(train_data)#replace missing values using the median along each column.\n",
        "train_columns_list=[i for i in range(len(train_data[0])) if i not in [0,1,3,4]]#a columns list for training data(without RID, DXCHANGE, MMSE, ADAS13)\n",
        "x=StandardScaler().fit_transform(train_data[:, train_columns_list])#an ndarray for training data #index 699 row of the initial data order has some median data.\n",
        "y=train_data[:, [3, 4]]# an ndarray for target data(MMSE, ADAS13)\n",
        "dxchange=train_data[:,1]# an array for target data(DXCHANGE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iu8Ur3Cyhhw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "baa320f9-7db2-4abd-87e3-526c3a81c4f9"
      },
      "source": [
        "'''\n",
        "since non-polynomial linear regression had the best score for prediction among polynomial or non-polynomial linear and sgd regression,\n",
        "I used linear regression for MMSE and adas13 score.\n",
        "'''\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "lin_reg=LinearRegression()#making linear regression model\n",
        "lin_reg.fit(x,y)#fitting train data and target\n",
        "#10 fold cross validation score\n",
        "print('cross validation score: ',np.sqrt(-cross_val_score(lin_reg, x, y, scoring=\"neg_mean_squared_error\", cv=10)).mean())#since the test data is 20% of the total data, I chose k=10.\n",
        "'''\n",
        "x, y linear regression cross_val: 5.27733401244674\n",
        "mmse linear reg cross_val: 2.16891415238337, score: 0.41485347954480956\n",
        "adas13 lin reg cross_val: 7.140578973138697, score: 0.484777196517327\n",
        "#####################################################################################################\n",
        "\n",
        "mmse_SGDRegressor(alpha=0.1, average=False, early_stopping=True, eta0=0.01, l1_ratio=0.4,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', shuffle=True, validation_fraction=0.2)\n",
        "            cross_val: 2.1448360763154763 , score: 0.37930080557313506\n",
        "\n",
        "adas13_SGDRegressor(alpha=0.3, average=False, early_stopping=True, eta0=0.1, l1_ratio=0,\n",
        "            learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "            n_iter_no_change=5, penalty='elasticnet', shuffle=True, validation_fraction=0.2)\n",
        "            cross_val: 7.110935165181526, score: 0.4647449371807022\n",
        "###################################################################################################\n",
        "\n",
        "mmse_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.005, fit_intercept=True, l1_ratio=0.1,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "            cross_val: 2.143334672668816, score: 0.3968742861103888\n",
        "\n",
        "adas13_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.01, fit_intercept=True, l1_ratio=0.4,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "             cross_val: 7.068923905893371, score: 0.4731238425894331\n",
        "##################################################################################################\n",
        "\n",
        "mmse_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.007, fit_intercept=True, l1_ratio=0.2,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "             cross_val: 2.1594286831952565 , score: 0.3955886718963466\n",
        "\n",
        "adas13_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.003, fit_intercept=True, l1_ratio=0.4,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "             cross_val: 7.1153570006336935 , score: 0.4732709028728066\n",
        "################################################################################################\n",
        "\n",
        "mmse_SGDRegressor(alpha=0.3, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.005, fit_intercept=True, l1_ratio=0.1,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "             cross_val: 2.143473713335877 , score:  0.37867148737035894\n",
        "\n",
        "adas13_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.01, fit_intercept=True, l1_ratio=0.2,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "             cross_val: 7.07580935097248 , score: 0.47477621211427457\n",
        "###############################################################################################\n",
        "\n",
        "mmse_SGDRegressor(alpha=0.2, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.005, fit_intercept=True, l1_ratio=0.1,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "            cross_val: 2.1443922431138387 , score: 0.3913663882716516\n",
        "\n",
        "adas13_SGDRegressor(alpha=0.2, average=False, early_stopping=True, epsilon=0.1,\n",
        "             eta0=0.007, fit_intercept=True, l1_ratio=0.1,\n",
        "             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\n",
        "             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\n",
        "             random_state=None, shuffle=True, tol=0.001,\n",
        "             validation_fraction=0.2, verbose=0, warm_start=False)\n",
        "             cross_val: 7.070435704050335 , score: 0.46752382605657083\n",
        "#################################################################################################\n",
        "\n",
        "ElasticNet(alpha=0.15, copy_X=True, fit_intercept=True, l1_ratio=0.2,\n",
        "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
        "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
        "           cross_val: 5.2145189475019125 , score: 0.4699015488222758\n",
        "\n",
        "above models and scores are just some of the things I've tried.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cross validation score:  5.27733401244674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nx, y linear regression cross_val: 5.27733401244674\\nmmse linear reg cross_val: 2.16891415238337, score: 0.41485347954480956\\nadas13 lin reg cross_val: 7.140578973138697, score: 0.484777196517327\\n#####################################################################################################\\n\\nmmse_SGDRegressor(alpha=0.1, average=False, early_stopping=True, eta0=0.01, l1_ratio=0.4,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', shuffle=True, validation_fraction=0.2) \\n            cross_val: 2.1448360763154763 , score: 0.37930080557313506\\n\\nadas13_SGDRegressor(alpha=0.3, average=False, early_stopping=True, eta0=0.1, l1_ratio=0, \\n            learning_rate='adaptive', loss='squared_loss', max_iter=1000, \\n            n_iter_no_change=5, penalty='elasticnet', shuffle=True, validation_fraction=0.2)\\n            cross_val: 7.110935165181526, score: 0.4647449371807022\\n###################################################################################################\\n\\nmmse_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.005, fit_intercept=True, l1_ratio=0.1,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n            cross_val: 2.143334672668816, score: 0.3968742861103888\\n\\nadas13_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.01, fit_intercept=True, l1_ratio=0.4,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n             cross_val: 7.068923905893371, score: 0.4731238425894331\\n##################################################################################################\\n\\nmmse_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.007, fit_intercept=True, l1_ratio=0.2,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n             cross_val: 2.1594286831952565 , score: 0.3955886718963466\\n\\nadas13_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.003, fit_intercept=True, l1_ratio=0.4,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n             cross_val: 7.1153570006336935 , score: 0.4732709028728066\\n################################################################################################\\n\\nmmse_SGDRegressor(alpha=0.3, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.005, fit_intercept=True, l1_ratio=0.1,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n             cross_val: 2.143473713335877 , score:  0.37867148737035894\\n\\nadas13_SGDRegressor(alpha=0.1, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.01, fit_intercept=True, l1_ratio=0.2,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n             cross_val: 7.07580935097248 , score: 0.47477621211427457          \\n###############################################################################################\\n\\nmmse_SGDRegressor(alpha=0.2, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.005, fit_intercept=True, l1_ratio=0.1,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n            cross_val: 2.1443922431138387 , score: 0.3913663882716516\\n\\nadas13_SGDRegressor(alpha=0.2, average=False, early_stopping=True, epsilon=0.1,\\n             eta0=0.007, fit_intercept=True, l1_ratio=0.1,\\n             learning_rate='adaptive', loss='squared_loss', max_iter=1000,\\n             n_iter_no_change=5, penalty='elasticnet', power_t=0.25,\\n             random_state=None, shuffle=True, tol=0.001,\\n             validation_fraction=0.2, verbose=0, warm_start=False)\\n             cross_val: 7.070435704050335 , score: 0.46752382605657083\\n#################################################################################################\\n\\nElasticNet(alpha=0.15, copy_X=True, fit_intercept=True, l1_ratio=0.2,\\n           max_iter=1000, normalize=False, positive=False, precompute=False,\\n           random_state=None, selection='cyclic', tol=0.0001, warm_start=False) \\n           cross_val: 5.2145189475019125 , score: 0.4699015488222758               \\n\\nabove models and scores are just some of the things I've tried.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZgxqqalyl1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "8939a6c2-9f0f-4b03-e54f-de1bcbf518e4"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def brighter_is_worser(real, predict):# this function is for showing confusion matrix\n",
        "  conf_mx=confusion_matrix(real, predict)\n",
        "  print('confusion matrix')\n",
        "  plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "  plt.show()\n",
        "  print('real row and predict column')\n",
        "  row_sums=conf_mx.sum(axis=1, keepdims=True)\n",
        "  norm_conf_mx=conf_mx/row_sums\n",
        "  np.fill_diagonal(norm_conf_mx, 0)\n",
        "  plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "  plt.show()\n",
        "#among polinomial or non-polinomial OvO sgd, OvA sgd, and softmax classification, OvA sgd classifier got the best score\n",
        "from sklearn.model_selection import GridSearchCV#I used grid search to find proper hyper parameter\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_clf=SGDClassifier(penalty='elasticnet', shuffle=True,\n",
        "                     learning_rate='adaptive', eta0=0.1, early_stopping=True,\n",
        "                     validation_fraction=0.2)\n",
        "np.random.seed(42)\n",
        "random=np.random.rand(20)\n",
        "param_grid=[{'alpha':list(random[:10]), 'l1_ratio':list(random[10:])}]\n",
        "\n",
        "grid_search=GridSearchCV(sgd_clf, param_grid, cv=10,\n",
        "                              scoring='neg_mean_squared_error', return_train_score=True)\n",
        "grid_search.fit(x,dxchange)\n",
        "sgd_clf=grid_search.best_estimator_\n",
        "sgd_clf.fit(x,dxchange)#Avg. loss: 0.274994\n",
        "print('score: ',sgd_clf.score(x, dxchange))#0.5715333822450477\n",
        "'''SGDClassifier(alpha=0.15599452033620265, average=False, class_weight=None,\n",
        "              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.020584494295802447, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\n",
        "              warm_start=False)'''\n",
        "brighter_is_worser(dxchange, cross_val_predict(sgd_clf, x, dxchange, cv=10))\n",
        "\n",
        "'''below models are what I tried. Though the models are named as reg, it is classification model. It is my mistake.\n",
        "\n",
        "sgd_reg=SGDClassifier(penalty='elasticnet', alpha=0.1, shuffle=True, #l1_ratio\n",
        "                     learning_rate='adaptive', eta0=0.1, early_stopping=True,\n",
        "                     validation_fraction=0.2, verbose=1)\n",
        "sgd_reg.fit(x, dxchange) #loss: 0.285239\n",
        "sgd_reg.score(x, dxchange) #0.5451210564930301\n",
        "########################################################################################################\n",
        "\n",
        "sgd_reg=SGDClassifier(alpha=1, average=False, class_weight=None, early_stopping=True,\n",
        "              epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.5488135039273248, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\n",
        "              warm_start=False)\n",
        "sgd_reg.fit(x,dxchange) #loss:0.306676\n",
        "sgd_reg.score(x, dxchange) #0.5077035950110051\n",
        "##########################################################################################################\n",
        "\n",
        "sgd_reg=SGDClassifier(alpha=0.21167117460322915, average=False, class_weight=None,\n",
        "              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.08076327655085713, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\n",
        "              warm_start=False).fit(x,dxchange)\n",
        "#Avg. loss: 0.303415\n",
        "###########################################################################################################\n",
        "\n",
        "SGDClassifier(alpha=0.16911083656253545, average=False, class_weight=None,\n",
        "              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.04689631938924976, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\n",
        "              warm_start=False).fit(x, dxchange)\n",
        "#Avg. loss: 0.289477, score: 0.5451210564930301\n",
        "########################################################################################################\n",
        "\n",
        "SGDClassifier(alpha=0.3745401188473625, average=False, class_weight=None,\n",
        "              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.15599452033620265, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=0,\n",
        "              warm_start=False)\n",
        "#loss: 0.306677, score:0.5077035950110051\n",
        "#######################################################################################################\n",
        "\n",
        "SGDClassifier(alpha=0.07546301953822482, average=False, class_weight=None,\n",
        "              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.1543787989679427, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=0,\n",
        "              warm_start=False)\n",
        "#loss:0.292431\n",
        "######################################################################################################\n",
        "\n",
        "SGDClassifier(alpha=0.15, average=False, class_weight=None, early_stopping=True,\n",
        "              epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.05,\n",
        "              learning_rate='adaptive', loss='hinge', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
        "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
        "              validation_fraction=0.2, verbose=1, warm_start=False).fit(x, dxchange)\n",
        "#loss: 0.287940\n",
        "###############################################################################################\n",
        "\n",
        "SGDClassifier(alpha=0.7, average=False, class_weight=None, early_stopping=True,\n",
        "              epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0,\n",
        "              learning_rate='adaptive', loss='hinge', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
        "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
        "              validation_fraction=0.2, verbose=1, warm_start=False)\n",
        "#Avg. loss: 0.292289, score:  0.516507703595011\n",
        "#################################################################################################\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score:  0.5715333822450477\n",
            "confusion matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "real row and predict column\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"below models are what I tried. Though the models are named as reg, it is classification model. It is my mistake.\\n\\nsgd_reg=SGDClassifier(penalty='elasticnet', alpha=0.1, shuffle=True, #l1_ratio\\n                     learning_rate='adaptive', eta0=0.1, early_stopping=True, \\n                     validation_fraction=0.2, verbose=1)\\nsgd_reg.fit(x, dxchange) #loss: 0.285239\\nsgd_reg.score(x, dxchange) #0.5451210564930301\\n########################################################################################################\\n\\nsgd_reg=SGDClassifier(alpha=1, average=False, class_weight=None, early_stopping=True,\\n              epsilon=0.1, eta0=0.1, fit_intercept=True,\\n              l1_ratio=0.5488135039273248, learning_rate='adaptive',\\n              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\\n              penalty='elasticnet', power_t=0.5, random_state=None,\\n              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\\n              warm_start=False)\\nsgd_reg.fit(x,dxchange) #loss:0.306676\\nsgd_reg.score(x, dxchange) #0.5077035950110051\\n##########################################################################################################\\n\\nsgd_reg=SGDClassifier(alpha=0.21167117460322915, average=False, class_weight=None,  \\n              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\\n              l1_ratio=0.08076327655085713, learning_rate='adaptive',\\n              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\\n              penalty='elasticnet', power_t=0.5, random_state=None,\\n              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\\n              warm_start=False).fit(x,dxchange)\\n#Avg. loss: 0.303415\\n###########################################################################################################\\n\\nSGDClassifier(alpha=0.16911083656253545, average=False, class_weight=None, \\n              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\\n              l1_ratio=0.04689631938924976, learning_rate='adaptive',\\n              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\\n              penalty='elasticnet', power_t=0.5, random_state=None,\\n              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=1,\\n              warm_start=False).fit(x, dxchange)\\n#Avg. loss: 0.289477, score: 0.5451210564930301              \\n########################################################################################################\\n\\nSGDClassifier(alpha=0.3745401188473625, average=False, class_weight=None,\\n              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\\n              l1_ratio=0.15599452033620265, learning_rate='adaptive',\\n              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\\n              penalty='elasticnet', power_t=0.5, random_state=None,\\n              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=0,\\n              warm_start=False)\\n#loss: 0.306677, score:0.5077035950110051\\n#######################################################################################################\\n\\nSGDClassifier(alpha=0.07546301953822482, average=False, class_weight=None,\\n              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\\n              l1_ratio=0.1543787989679427, learning_rate='adaptive',\\n              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\\n              penalty='elasticnet', power_t=0.5, random_state=None,\\n              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=0,\\n              warm_start=False)\\n#loss:0.292431\\n######################################################################################################\\n\\nSGDClassifier(alpha=0.15, average=False, class_weight=None, early_stopping=True,\\n              epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.05,\\n              learning_rate='adaptive', loss='hinge', max_iter=1000,\\n              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\\n              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\\n              validation_fraction=0.2, verbose=1, warm_start=False).fit(x, dxchange)\\n#loss: 0.287940\\n###############################################################################################\\n\\nSGDClassifier(alpha=0.7, average=False, class_weight=None, early_stopping=True,\\n              epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0,\\n              learning_rate='adaptive', loss='hinge', max_iter=1000,\\n              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\\n              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\\n              validation_fraction=0.2, verbose=1, warm_start=False)\\n#Avg. loss: 0.292289, score:  0.516507703595011\\n#################################################################################################\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT8KgGRqyfjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "01da9c6e-906e-454c-abe6-951170fa468e"
      },
      "source": [
        "# Load test dataset\n",
        "csv_file_test = '/content/gdrive/My Drive/BNCS401_Midterm_Project/Test_data_ageupdated.csv'  # Set your path\n",
        "test_data = pd.read_csv(csv_file_test)\n",
        "test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RID</th>\n",
              "      <th>AGE</th>\n",
              "      <th>ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1002</td>\n",
              "      <td>76.3</td>\n",
              "      <td>1.996</td>\n",
              "      <td>2.220</td>\n",
              "      <td>2.342</td>\n",
              "      <td>2.803</td>\n",
              "      <td>2.230</td>\n",
              "      <td>1.402</td>\n",
              "      <td>1.764</td>\n",
              "      <td>2.156</td>\n",
              "      <td>2.026</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.964</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.454</td>\n",
              "      <td>1.925</td>\n",
              "      <td>2.451</td>\n",
              "      <td>2.311</td>\n",
              "      <td>3.016</td>\n",
              "      <td>2.317</td>\n",
              "      <td>1.121</td>\n",
              "      <td>2.956</td>\n",
              "      <td>2.936</td>\n",
              "      <td>2.219</td>\n",
              "      <td>2.907</td>\n",
              "      <td>2.320</td>\n",
              "      <td>1.573</td>\n",
              "      <td>3.061</td>\n",
              "      <td>2.696</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.283</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.686</td>\n",
              "      <td>1.768</td>\n",
              "      <td>2.387</td>\n",
              "      <td>2.674</td>\n",
              "      <td>1.819</td>\n",
              "      <td>2.135</td>\n",
              "      <td>2.133</td>\n",
              "      <td>3.003</td>\n",
              "      <td>2.374</td>\n",
              "      <td>1.411</td>\n",
              "      <td>1.727</td>\n",
              "      <td>2.434</td>\n",
              "      <td>2.157</td>\n",
              "      <td>2.063</td>\n",
              "      <td>3.182</td>\n",
              "      <td>2.085</td>\n",
              "      <td>2.485</td>\n",
              "      <td>1.905</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.182</td>\n",
              "      <td>3.563</td>\n",
              "      <td>2.187</td>\n",
              "      <td>0.975</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.253</td>\n",
              "      <td>2.476</td>\n",
              "      <td>1.639</td>\n",
              "      <td>3.655</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.461</td>\n",
              "      <td>2.157</td>\n",
              "      <td>2.911</td>\n",
              "      <td>2.410</td>\n",
              "      <td>1.968</td>\n",
              "      <td>2.800</td>\n",
              "      <td>1.767</td>\n",
              "      <td>2.080</td>\n",
              "      <td>2.766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>260</td>\n",
              "      <td>78.6</td>\n",
              "      <td>2.209</td>\n",
              "      <td>2.805</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.209</td>\n",
              "      <td>1.517</td>\n",
              "      <td>1.648</td>\n",
              "      <td>2.457</td>\n",
              "      <td>2.184</td>\n",
              "      <td>2.132</td>\n",
              "      <td>3.077</td>\n",
              "      <td>2.279</td>\n",
              "      <td>2.552</td>\n",
              "      <td>2.169</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.342</td>\n",
              "      <td>4.067</td>\n",
              "      <td>2.303</td>\n",
              "      <td>1.082</td>\n",
              "      <td>3.035</td>\n",
              "      <td>2.973</td>\n",
              "      <td>2.370</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.502</td>\n",
              "      <td>1.914</td>\n",
              "      <td>3.925</td>\n",
              "      <td>2.438</td>\n",
              "      <td>2.820</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.932</td>\n",
              "      <td>2.328</td>\n",
              "      <td>2.134</td>\n",
              "      <td>2.831</td>\n",
              "      <td>1.838</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.219</td>\n",
              "      <td>2.797</td>\n",
              "      <td>2.456</td>\n",
              "      <td>2.797</td>\n",
              "      <td>2.147</td>\n",
              "      <td>1.469</td>\n",
              "      <td>1.729</td>\n",
              "      <td>2.692</td>\n",
              "      <td>2.329</td>\n",
              "      <td>2.274</td>\n",
              "      <td>2.994</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.627</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.458</td>\n",
              "      <td>2.314</td>\n",
              "      <td>3.839</td>\n",
              "      <td>2.167</td>\n",
              "      <td>1.228</td>\n",
              "      <td>2.366</td>\n",
              "      <td>2.834</td>\n",
              "      <td>2.320</td>\n",
              "      <td>1.607</td>\n",
              "      <td>3.849</td>\n",
              "      <td>2.843</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.484</td>\n",
              "      <td>2.811</td>\n",
              "      <td>2.402</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.685</td>\n",
              "      <td>1.795</td>\n",
              "      <td>2.332</td>\n",
              "      <td>2.965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1406</td>\n",
              "      <td>61.1</td>\n",
              "      <td>2.114</td>\n",
              "      <td>3.157</td>\n",
              "      <td>2.700</td>\n",
              "      <td>2.856</td>\n",
              "      <td>2.463</td>\n",
              "      <td>1.610</td>\n",
              "      <td>1.671</td>\n",
              "      <td>2.720</td>\n",
              "      <td>2.174</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.997</td>\n",
              "      <td>2.353</td>\n",
              "      <td>2.818</td>\n",
              "      <td>1.856</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.372</td>\n",
              "      <td>3.734</td>\n",
              "      <td>2.217</td>\n",
              "      <td>1.286</td>\n",
              "      <td>3.094</td>\n",
              "      <td>3.091</td>\n",
              "      <td>2.362</td>\n",
              "      <td>2.674</td>\n",
              "      <td>2.513</td>\n",
              "      <td>1.653</td>\n",
              "      <td>3.764</td>\n",
              "      <td>2.984</td>\n",
              "      <td>2.735</td>\n",
              "      <td>2.188</td>\n",
              "      <td>2.823</td>\n",
              "      <td>2.546</td>\n",
              "      <td>1.992</td>\n",
              "      <td>2.768</td>\n",
              "      <td>1.905</td>\n",
              "      <td>2.517</td>\n",
              "      <td>2.869</td>\n",
              "      <td>2.010</td>\n",
              "      <td>3.147</td>\n",
              "      <td>2.558</td>\n",
              "      <td>2.763</td>\n",
              "      <td>2.175</td>\n",
              "      <td>1.532</td>\n",
              "      <td>1.609</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.088</td>\n",
              "      <td>2.174</td>\n",
              "      <td>3.211</td>\n",
              "      <td>2.401</td>\n",
              "      <td>2.840</td>\n",
              "      <td>1.966</td>\n",
              "      <td>2.815</td>\n",
              "      <td>2.254</td>\n",
              "      <td>3.713</td>\n",
              "      <td>2.452</td>\n",
              "      <td>1.164</td>\n",
              "      <td>2.778</td>\n",
              "      <td>2.551</td>\n",
              "      <td>2.402</td>\n",
              "      <td>1.810</td>\n",
              "      <td>3.646</td>\n",
              "      <td>3.352</td>\n",
              "      <td>2.766</td>\n",
              "      <td>2.249</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.401</td>\n",
              "      <td>1.983</td>\n",
              "      <td>2.666</td>\n",
              "      <td>1.890</td>\n",
              "      <td>2.926</td>\n",
              "      <td>2.903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>516</td>\n",
              "      <td>87.6</td>\n",
              "      <td>1.961</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.732</td>\n",
              "      <td>2.494</td>\n",
              "      <td>1.317</td>\n",
              "      <td>1.613</td>\n",
              "      <td>2.371</td>\n",
              "      <td>2.045</td>\n",
              "      <td>2.083</td>\n",
              "      <td>2.429</td>\n",
              "      <td>2.243</td>\n",
              "      <td>2.563</td>\n",
              "      <td>1.940</td>\n",
              "      <td>2.612</td>\n",
              "      <td>2.379</td>\n",
              "      <td>4.150</td>\n",
              "      <td>2.147</td>\n",
              "      <td>1.147</td>\n",
              "      <td>2.976</td>\n",
              "      <td>2.947</td>\n",
              "      <td>2.079</td>\n",
              "      <td>3.115</td>\n",
              "      <td>2.432</td>\n",
              "      <td>1.599</td>\n",
              "      <td>3.674</td>\n",
              "      <td>1.981</td>\n",
              "      <td>2.547</td>\n",
              "      <td>2.228</td>\n",
              "      <td>2.755</td>\n",
              "      <td>2.501</td>\n",
              "      <td>2.031</td>\n",
              "      <td>2.635</td>\n",
              "      <td>1.751</td>\n",
              "      <td>2.175</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.430</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.485</td>\n",
              "      <td>2.659</td>\n",
              "      <td>2.382</td>\n",
              "      <td>1.270</td>\n",
              "      <td>1.718</td>\n",
              "      <td>2.125</td>\n",
              "      <td>2.048</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.853</td>\n",
              "      <td>2.295</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.049</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.457</td>\n",
              "      <td>4.151</td>\n",
              "      <td>2.374</td>\n",
              "      <td>1.062</td>\n",
              "      <td>2.409</td>\n",
              "      <td>2.051</td>\n",
              "      <td>2.259</td>\n",
              "      <td>1.478</td>\n",
              "      <td>3.491</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.567</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.830</td>\n",
              "      <td>2.464</td>\n",
              "      <td>2.032</td>\n",
              "      <td>2.497</td>\n",
              "      <td>1.730</td>\n",
              "      <td>2.279</td>\n",
              "      <td>2.825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4138</td>\n",
              "      <td>88.4</td>\n",
              "      <td>2.246</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.850</td>\n",
              "      <td>2.566</td>\n",
              "      <td>1.770</td>\n",
              "      <td>1.921</td>\n",
              "      <td>2.398</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.200</td>\n",
              "      <td>2.588</td>\n",
              "      <td>2.397</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.038</td>\n",
              "      <td>2.323</td>\n",
              "      <td>2.396</td>\n",
              "      <td>3.350</td>\n",
              "      <td>2.415</td>\n",
              "      <td></td>\n",
              "      <td>3.084</td>\n",
              "      <td>3.030</td>\n",
              "      <td>2.306</td>\n",
              "      <td>2.816</td>\n",
              "      <td>2.366</td>\n",
              "      <td>1.930</td>\n",
              "      <td>2.580</td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.292</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.355</td>\n",
              "      <td>2.147</td>\n",
              "      <td>2.626</td>\n",
              "      <td>1.933</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.350</td>\n",
              "      <td>2.913</td>\n",
              "      <td>2.251</td>\n",
              "      <td>2.799</td>\n",
              "      <td>2.463</td>\n",
              "      <td>1.671</td>\n",
              "      <td>1.967</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.358</td>\n",
              "      <td>3.378</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.607</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.358</td>\n",
              "      <td>2.253</td>\n",
              "      <td>3.548</td>\n",
              "      <td>2.239</td>\n",
              "      <td></td>\n",
              "      <td>2.401</td>\n",
              "      <td>2.665</td>\n",
              "      <td>2.497</td>\n",
              "      <td>1.858</td>\n",
              "      <td>2.393</td>\n",
              "      <td>2.765</td>\n",
              "      <td>2.452</td>\n",
              "      <td>2.327</td>\n",
              "      <td>2.644</td>\n",
              "      <td>1.992</td>\n",
              "      <td>2.141</td>\n",
              "      <td>2.895</td>\n",
              "      <td>1.838</td>\n",
              "      <td>2.533</td>\n",
              "      <td>2.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>1090</td>\n",
              "      <td>71.3</td>\n",
              "      <td>1.918</td>\n",
              "      <td>2.442</td>\n",
              "      <td>2.224</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.068</td>\n",
              "      <td>1.455</td>\n",
              "      <td>1.771</td>\n",
              "      <td>2.435</td>\n",
              "      <td>2.062</td>\n",
              "      <td>2.075</td>\n",
              "      <td>3.014</td>\n",
              "      <td>2.208</td>\n",
              "      <td>2.472</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.526</td>\n",
              "      <td>2.051</td>\n",
              "      <td>3.770</td>\n",
              "      <td>2.354</td>\n",
              "      <td>1.024</td>\n",
              "      <td>2.962</td>\n",
              "      <td>3.011</td>\n",
              "      <td>2.239</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.186</td>\n",
              "      <td>1.810</td>\n",
              "      <td>3.251</td>\n",
              "      <td>2.831</td>\n",
              "      <td>2.518</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.783</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.107</td>\n",
              "      <td>2.486</td>\n",
              "      <td>1.750</td>\n",
              "      <td>2.213</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.804</td>\n",
              "      <td>2.300</td>\n",
              "      <td>2.447</td>\n",
              "      <td>2.342</td>\n",
              "      <td>1.355</td>\n",
              "      <td>1.825</td>\n",
              "      <td>2.621</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.206</td>\n",
              "      <td>2.870</td>\n",
              "      <td>2.247</td>\n",
              "      <td>2.490</td>\n",
              "      <td>1.984</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.222</td>\n",
              "      <td>3.873</td>\n",
              "      <td>2.563</td>\n",
              "      <td>1.121</td>\n",
              "      <td>2.114</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.393</td>\n",
              "      <td>1.757</td>\n",
              "      <td>3.429</td>\n",
              "      <td>2.494</td>\n",
              "      <td>2.697</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.768</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.224</td>\n",
              "      <td>2.534</td>\n",
              "      <td>1.975</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>669</td>\n",
              "      <td>63.6</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.163</td>\n",
              "      <td>2.472</td>\n",
              "      <td>2.511</td>\n",
              "      <td>2.524</td>\n",
              "      <td>1.377</td>\n",
              "      <td>1.759</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.091</td>\n",
              "      <td>2.039</td>\n",
              "      <td>2.957</td>\n",
              "      <td>2.117</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.477</td>\n",
              "      <td>2.414</td>\n",
              "      <td>3.278</td>\n",
              "      <td>2.404</td>\n",
              "      <td>1.212</td>\n",
              "      <td>2.880</td>\n",
              "      <td>2.832</td>\n",
              "      <td>2.404</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.241</td>\n",
              "      <td>1.813</td>\n",
              "      <td>3.050</td>\n",
              "      <td>2.773</td>\n",
              "      <td>2.593</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.295</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.440</td>\n",
              "      <td>1.753</td>\n",
              "      <td>2.145</td>\n",
              "      <td>2.833</td>\n",
              "      <td>2.042</td>\n",
              "      <td>2.673</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.551</td>\n",
              "      <td>2.293</td>\n",
              "      <td>1.380</td>\n",
              "      <td>1.695</td>\n",
              "      <td>2.481</td>\n",
              "      <td>1.954</td>\n",
              "      <td>2.283</td>\n",
              "      <td>2.694</td>\n",
              "      <td>2.165</td>\n",
              "      <td>2.385</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.491</td>\n",
              "      <td>2.223</td>\n",
              "      <td>3.438</td>\n",
              "      <td>1.753</td>\n",
              "      <td>1.14</td>\n",
              "      <td>2.348</td>\n",
              "      <td>2.402</td>\n",
              "      <td>2.423</td>\n",
              "      <td>1.636</td>\n",
              "      <td>3.465</td>\n",
              "      <td>2.214</td>\n",
              "      <td>2.244</td>\n",
              "      <td>2.476</td>\n",
              "      <td>2.579</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.028</td>\n",
              "      <td>2.492</td>\n",
              "      <td>1.702</td>\n",
              "      <td>2.026</td>\n",
              "      <td>2.627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>298</td>\n",
              "      <td>76.4</td>\n",
              "      <td>2.075</td>\n",
              "      <td>2.326</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.815</td>\n",
              "      <td>2.251</td>\n",
              "      <td>1.225</td>\n",
              "      <td>1.683</td>\n",
              "      <td>2.356</td>\n",
              "      <td>2.330</td>\n",
              "      <td>2.151</td>\n",
              "      <td>2.694</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.642</td>\n",
              "      <td>2.100</td>\n",
              "      <td>2.776</td>\n",
              "      <td>2.503</td>\n",
              "      <td>3.833</td>\n",
              "      <td>2.586</td>\n",
              "      <td>1.197</td>\n",
              "      <td>2.903</td>\n",
              "      <td>2.775</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.420</td>\n",
              "      <td>2.425</td>\n",
              "      <td>1.830</td>\n",
              "      <td>3.475</td>\n",
              "      <td>2.861</td>\n",
              "      <td>2.727</td>\n",
              "      <td>2.437</td>\n",
              "      <td>2.688</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.645</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.761</td>\n",
              "      <td>2.073</td>\n",
              "      <td>2.741</td>\n",
              "      <td>2.430</td>\n",
              "      <td>2.585</td>\n",
              "      <td>2.093</td>\n",
              "      <td>1.326</td>\n",
              "      <td>1.910</td>\n",
              "      <td>2.308</td>\n",
              "      <td>2.453</td>\n",
              "      <td>2.163</td>\n",
              "      <td>3.024</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.626</td>\n",
              "      <td>2.169</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.523</td>\n",
              "      <td>3.883</td>\n",
              "      <td>2.259</td>\n",
              "      <td>1.247</td>\n",
              "      <td>2.341</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.450</td>\n",
              "      <td>1.601</td>\n",
              "      <td>4.016</td>\n",
              "      <td>2.652</td>\n",
              "      <td>2.542</td>\n",
              "      <td>2.367</td>\n",
              "      <td>2.747</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.636</td>\n",
              "      <td>1.732</td>\n",
              "      <td>2.416</td>\n",
              "      <td>2.827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>890</td>\n",
              "      <td>59.7</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.552</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.912</td>\n",
              "      <td>2.478</td>\n",
              "      <td>1.541</td>\n",
              "      <td>1.980</td>\n",
              "      <td>2.443</td>\n",
              "      <td>2.442</td>\n",
              "      <td>2.222</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.287</td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.125</td>\n",
              "      <td>2.727</td>\n",
              "      <td>2.314</td>\n",
              "      <td>3.879</td>\n",
              "      <td>2.699</td>\n",
              "      <td>1.087</td>\n",
              "      <td>2.876</td>\n",
              "      <td>2.833</td>\n",
              "      <td>1.952</td>\n",
              "      <td>2.601</td>\n",
              "      <td>2.544</td>\n",
              "      <td>1.796</td>\n",
              "      <td>3.235</td>\n",
              "      <td>2.810</td>\n",
              "      <td>2.884</td>\n",
              "      <td>2.210</td>\n",
              "      <td>2.788</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.172</td>\n",
              "      <td>2.634</td>\n",
              "      <td>1.860</td>\n",
              "      <td>2.475</td>\n",
              "      <td>2.550</td>\n",
              "      <td>2.264</td>\n",
              "      <td>3.112</td>\n",
              "      <td>2.547</td>\n",
              "      <td>2.667</td>\n",
              "      <td>2.316</td>\n",
              "      <td>1.534</td>\n",
              "      <td>1.814</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.510</td>\n",
              "      <td>2.141</td>\n",
              "      <td>2.997</td>\n",
              "      <td>2.156</td>\n",
              "      <td>2.686</td>\n",
              "      <td>2.119</td>\n",
              "      <td>2.773</td>\n",
              "      <td>2.316</td>\n",
              "      <td>3.747</td>\n",
              "      <td>2.884</td>\n",
              "      <td>1.092</td>\n",
              "      <td>2.364</td>\n",
              "      <td>2.541</td>\n",
              "      <td>2.576</td>\n",
              "      <td>1.701</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.868</td>\n",
              "      <td>2.752</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.069</td>\n",
              "      <td>2.902</td>\n",
              "      <td>1.890</td>\n",
              "      <td>2.306</td>\n",
              "      <td>2.782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>1321</td>\n",
              "      <td>83.2</td>\n",
              "      <td>2.284</td>\n",
              "      <td>2.613</td>\n",
              "      <td>2.459</td>\n",
              "      <td>2.820</td>\n",
              "      <td>2.186</td>\n",
              "      <td>1.356</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.316</td>\n",
              "      <td>2.366</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.459</td>\n",
              "      <td>2.212</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.129</td>\n",
              "      <td>2.617</td>\n",
              "      <td>2.524</td>\n",
              "      <td>3.621</td>\n",
              "      <td>2.262</td>\n",
              "      <td>1.097</td>\n",
              "      <td>3.078</td>\n",
              "      <td>3.040</td>\n",
              "      <td>2.492</td>\n",
              "      <td>2.838</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.648</td>\n",
              "      <td>3.036</td>\n",
              "      <td>3.021</td>\n",
              "      <td>2.472</td>\n",
              "      <td>2.347</td>\n",
              "      <td>2.704</td>\n",
              "      <td>2.651</td>\n",
              "      <td>2.078</td>\n",
              "      <td>2.683</td>\n",
              "      <td>1.779</td>\n",
              "      <td>2.388</td>\n",
              "      <td>2.768</td>\n",
              "      <td>2.173</td>\n",
              "      <td>2.519</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.557</td>\n",
              "      <td>2.287</td>\n",
              "      <td>1.283</td>\n",
              "      <td>1.999</td>\n",
              "      <td>2.719</td>\n",
              "      <td>2.417</td>\n",
              "      <td>2.205</td>\n",
              "      <td>2.738</td>\n",
              "      <td>2.221</td>\n",
              "      <td>2.622</td>\n",
              "      <td>2.129</td>\n",
              "      <td>2.536</td>\n",
              "      <td>2.443</td>\n",
              "      <td>3.084</td>\n",
              "      <td>2.400</td>\n",
              "      <td>0.971</td>\n",
              "      <td>2.358</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.704</td>\n",
              "      <td>1.691</td>\n",
              "      <td>3.388</td>\n",
              "      <td>2.716</td>\n",
              "      <td>2.526</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.829</td>\n",
              "      <td>2.473</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.572</td>\n",
              "      <td>1.758</td>\n",
              "      <td>2.164</td>\n",
              "      <td>2.832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>344 rows × 72 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RID  ...  ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
              "0    1002  ...                                       2.766\n",
              "1     260  ...                                       2.965\n",
              "2    1406  ...                                       2.903\n",
              "3     516  ...                                       2.825\n",
              "4    4138  ...                                       2.777\n",
              "..    ...  ...                                         ...\n",
              "339  1090  ...                                       2.622\n",
              "340   669  ...                                       2.627\n",
              "341   298  ...                                       2.827\n",
              "342   890  ...                                       2.782\n",
              "343  1321  ...                                       2.832\n",
              "\n",
              "[344 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CccY12kgjRnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "aa7796e9-5d8e-411f-87d2-f41fb6a3f676"
      },
      "source": [
        "# Sample code to save the csv file into Google Drive\n",
        "for i in test_data.columns:#since there are object type data and null, it needs to be changed.\n",
        "  if test_data[i].dtype=='object':#for object type data column.\n",
        "    test_data[i]=pd.to_numeric(test_data[i], errors='coerce')#numeric entries are changed to float type and missing entries are replaced with NaN.\n",
        "  if test_data[i].isnull().any():#missing entries are replaced with NaN.\n",
        "    test_data[i].loc[test_data[i].isnull()]=np.NaN\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "test_data= SimpleImputer(strategy='median').fit_transform(test_data)\n",
        "RID=test_data[:, 0]# store RID\n",
        "x=StandardScaler().fit_transform(test_data[:, 1:])#making data\n",
        "\n",
        "reg=lin_reg.predict(x)# predict MMSE and ADAS13\n",
        "clf=sgd_clf.predict(x)# predict DXCHANGE\n",
        "data=[[RID[i], reg[i][0], reg[i][1], clf[i]] for i in range(len(RID))]\n",
        "test_prediction = pd.DataFrame(data, columns = ['RID', 'MMSE', 'ADAS13', 'DXCHANGE'])\n",
        "test_prediction.to_csv('/content/gdrive/My Drive/Test_prediction.csv', index=False)\n",
        "!cp Test_prediction.csv /content/gdrive/My Drive/BNCS401_Midterm_Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cp: target 'Drive/BNCS401_Midterm_Project' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}