{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongSeong/brainML_HW/blob/main/Neural_Networks_for_Multi_task_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wStZFhkP0oiW"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL-WIKzpphlK"
      },
      "source": [
        "**Load TADPOLE* dataset (csv file) from Google Drive**\n",
        "-------------------------------------------------------\n",
        "*The Alzheimer's Disease Prediction Of Longitudinal Evolution\n",
        "(https://tadpole.grand-challenge.org/)\n",
        "\n",
        "### -Subjects: 1707 (1363 Train (80%) + 344 Test (20%))\n",
        "\n",
        "\n",
        "### -Features: 72\n",
        "*   2 demographic feature: MMSE, ADAS13\n",
        "*   70 mean values of cortical thickness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS_uk0HaweJH",
        "outputId": "09646f06-eb84-4fee-ff91-51509b62ca78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "csv_file_train = '/content/gdrive/My Drive/BNCS401_Midterm_Project/Train_data_reupdated.csv'  # Set your path\n",
        "train_data = pd.read_csv(csv_file_train)\n",
        "train_data\n",
        "\n",
        "# DXCHANGE: clinical label (1-CN, 2-MCI, 3-AD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RID</th>\n",
              "      <th>DXCHANGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>MMSE</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4084</td>\n",
              "      <td>1</td>\n",
              "      <td>68.4</td>\n",
              "      <td>30</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2.700</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.613</td>\n",
              "      <td>2.904</td>\n",
              "      <td>2.311</td>\n",
              "      <td>1.647</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.652</td>\n",
              "      <td>2.604</td>\n",
              "      <td>2.480</td>\n",
              "      <td>3.095</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.207</td>\n",
              "      <td>2.903</td>\n",
              "      <td>2.617</td>\n",
              "      <td>4.117</td>\n",
              "      <td>2.701</td>\n",
              "      <td></td>\n",
              "      <td>3.127</td>\n",
              "      <td>3.051</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.872</td>\n",
              "      <td>2.732</td>\n",
              "      <td>2.026</td>\n",
              "      <td>3.756</td>\n",
              "      <td>2.813</td>\n",
              "      <td>2.762</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.695</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.690</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.421</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.370</td>\n",
              "      <td>2.674</td>\n",
              "      <td>3.004</td>\n",
              "      <td>2.369</td>\n",
              "      <td>1.599</td>\n",
              "      <td>2.208</td>\n",
              "      <td>2.650</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.544</td>\n",
              "      <td>3.018</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.880</td>\n",
              "      <td>2.322</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.489</td>\n",
              "      <td>3.620</td>\n",
              "      <td>2.711</td>\n",
              "      <td></td>\n",
              "      <td>2.593</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.660</td>\n",
              "      <td>1.993</td>\n",
              "      <td>3.734</td>\n",
              "      <td>2.390</td>\n",
              "      <td>2.817</td>\n",
              "      <td>2.471</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.667</td>\n",
              "      <td>2.490</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.171</td>\n",
              "      <td>2.862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2196</td>\n",
              "      <td>2</td>\n",
              "      <td>68.2</td>\n",
              "      <td>30</td>\n",
              "      <td>13.00</td>\n",
              "      <td>2.453</td>\n",
              "      <td>2.992</td>\n",
              "      <td>2.470</td>\n",
              "      <td>2.965</td>\n",
              "      <td>2.438</td>\n",
              "      <td>1.584</td>\n",
              "      <td>1.910</td>\n",
              "      <td>2.900</td>\n",
              "      <td>2.451</td>\n",
              "      <td>2.335</td>\n",
              "      <td>2.771</td>\n",
              "      <td>2.354</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.363</td>\n",
              "      <td>3.613</td>\n",
              "      <td>2.475</td>\n",
              "      <td></td>\n",
              "      <td>3.196</td>\n",
              "      <td>3.334</td>\n",
              "      <td>2.343</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.627</td>\n",
              "      <td>1.742</td>\n",
              "      <td>3.383</td>\n",
              "      <td>2.647</td>\n",
              "      <td>2.758</td>\n",
              "      <td>2.394</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.241</td>\n",
              "      <td>2.824</td>\n",
              "      <td>1.865</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.866</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.874</td>\n",
              "      <td>2.316</td>\n",
              "      <td>1.478</td>\n",
              "      <td>1.909</td>\n",
              "      <td>2.780</td>\n",
              "      <td>2.589</td>\n",
              "      <td>2.133</td>\n",
              "      <td>3.036</td>\n",
              "      <td>2.329</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.070</td>\n",
              "      <td>2.783</td>\n",
              "      <td>2.594</td>\n",
              "      <td>3.405</td>\n",
              "      <td>2.367</td>\n",
              "      <td></td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.977</td>\n",
              "      <td>2.489</td>\n",
              "      <td>1.868</td>\n",
              "      <td>3.220</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.372</td>\n",
              "      <td>2.854</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.233</td>\n",
              "      <td>2.793</td>\n",
              "      <td>1.987</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>657</td>\n",
              "      <td>1</td>\n",
              "      <td>77.7</td>\n",
              "      <td>29</td>\n",
              "      <td>15.33</td>\n",
              "      <td>2.249</td>\n",
              "      <td>2.296</td>\n",
              "      <td>2.315</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.420</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.327</td>\n",
              "      <td>2.193</td>\n",
              "      <td>2.415</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.559</td>\n",
              "      <td>2.008</td>\n",
              "      <td>2.495</td>\n",
              "      <td>2.418</td>\n",
              "      <td>4.210</td>\n",
              "      <td>2.167</td>\n",
              "      <td>1.08</td>\n",
              "      <td>3.362</td>\n",
              "      <td>3.077</td>\n",
              "      <td>2.648</td>\n",
              "      <td>2.759</td>\n",
              "      <td>2.442</td>\n",
              "      <td>1.717</td>\n",
              "      <td>2.886</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.737</td>\n",
              "      <td>2.402</td>\n",
              "      <td>3.012</td>\n",
              "      <td>2.418</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.805</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.318</td>\n",
              "      <td>3.025</td>\n",
              "      <td>2.270</td>\n",
              "      <td>1.831</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.908</td>\n",
              "      <td>2.335</td>\n",
              "      <td>1.389</td>\n",
              "      <td>1.765</td>\n",
              "      <td>2.397</td>\n",
              "      <td>2.187</td>\n",
              "      <td>2.188</td>\n",
              "      <td>2.889</td>\n",
              "      <td>2.318</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.028</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.309</td>\n",
              "      <td>3.880</td>\n",
              "      <td>1.832</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.343</td>\n",
              "      <td>2.363</td>\n",
              "      <td>1.601</td>\n",
              "      <td>3.683</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.385</td>\n",
              "      <td>2.365</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.415</td>\n",
              "      <td>2.252</td>\n",
              "      <td>2.583</td>\n",
              "      <td>1.850</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4526</td>\n",
              "      <td>3</td>\n",
              "      <td>79.4</td>\n",
              "      <td>22</td>\n",
              "      <td>24.00</td>\n",
              "      <td>2.197</td>\n",
              "      <td>2.289</td>\n",
              "      <td>2.258</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.124</td>\n",
              "      <td>1.473</td>\n",
              "      <td>1.729</td>\n",
              "      <td>2.680</td>\n",
              "      <td>2.184</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.680</td>\n",
              "      <td>2.218</td>\n",
              "      <td>2.407</td>\n",
              "      <td>1.923</td>\n",
              "      <td>2.542</td>\n",
              "      <td>2.296</td>\n",
              "      <td>3.135</td>\n",
              "      <td>2.157</td>\n",
              "      <td></td>\n",
              "      <td>2.671</td>\n",
              "      <td>2.925</td>\n",
              "      <td>2.288</td>\n",
              "      <td>3.322</td>\n",
              "      <td>2.336</td>\n",
              "      <td>1.691</td>\n",
              "      <td>2.399</td>\n",
              "      <td>2.940</td>\n",
              "      <td>2.178</td>\n",
              "      <td>2.079</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.083</td>\n",
              "      <td>1.904</td>\n",
              "      <td>2.400</td>\n",
              "      <td>1.750</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.493</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.244</td>\n",
              "      <td>2.399</td>\n",
              "      <td>2.308</td>\n",
              "      <td>2.073</td>\n",
              "      <td>1.506</td>\n",
              "      <td>1.777</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.319</td>\n",
              "      <td>2.087</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.410</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.269</td>\n",
              "      <td>3.085</td>\n",
              "      <td>2.080</td>\n",
              "      <td></td>\n",
              "      <td>2.503</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.372</td>\n",
              "      <td>1.757</td>\n",
              "      <td>2.651</td>\n",
              "      <td>2.665</td>\n",
              "      <td>2.578</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.537</td>\n",
              "      <td>1.958</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>362</td>\n",
              "      <td>2</td>\n",
              "      <td>70.5</td>\n",
              "      <td>24</td>\n",
              "      <td>20.33</td>\n",
              "      <td>1.765</td>\n",
              "      <td>2.081</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.461</td>\n",
              "      <td>2.140</td>\n",
              "      <td>1.466</td>\n",
              "      <td>1.749</td>\n",
              "      <td>2.025</td>\n",
              "      <td>2.021</td>\n",
              "      <td>1.938</td>\n",
              "      <td>2.266</td>\n",
              "      <td>2.114</td>\n",
              "      <td>2.368</td>\n",
              "      <td>1.811</td>\n",
              "      <td>2.227</td>\n",
              "      <td>2.191</td>\n",
              "      <td>3.188</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.013</td>\n",
              "      <td>2.548</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.280</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.291</td>\n",
              "      <td>1.821</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.248</td>\n",
              "      <td>2.414</td>\n",
              "      <td>2.164</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.012</td>\n",
              "      <td>1.916</td>\n",
              "      <td>2.451</td>\n",
              "      <td>1.959</td>\n",
              "      <td>2.519</td>\n",
              "      <td>2.405</td>\n",
              "      <td>1.968</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.201</td>\n",
              "      <td>2.711</td>\n",
              "      <td>2.287</td>\n",
              "      <td>1.753</td>\n",
              "      <td>1.644</td>\n",
              "      <td>2.209</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.745</td>\n",
              "      <td>2.102</td>\n",
              "      <td>2.519</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.099</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.191</td>\n",
              "      <td>0.922</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.273</td>\n",
              "      <td>1.662</td>\n",
              "      <td>2.790</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.348</td>\n",
              "      <td>2.197</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.844</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.121</td>\n",
              "      <td>1.951</td>\n",
              "      <td>2.262</td>\n",
              "      <td>2.245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>4187</td>\n",
              "      <td>2</td>\n",
              "      <td>62.0</td>\n",
              "      <td>29</td>\n",
              "      <td>17.00</td>\n",
              "      <td>2.441</td>\n",
              "      <td>2.813</td>\n",
              "      <td>2.614</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.354</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.395</td>\n",
              "      <td>2.923</td>\n",
              "      <td>2.005</td>\n",
              "      <td>2.571</td>\n",
              "      <td>2.312</td>\n",
              "      <td>2.923</td>\n",
              "      <td>2.596</td>\n",
              "      <td>3.972</td>\n",
              "      <td>2.793</td>\n",
              "      <td></td>\n",
              "      <td>3.396</td>\n",
              "      <td>3.230</td>\n",
              "      <td>2.417</td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.544</td>\n",
              "      <td>1.938</td>\n",
              "      <td>3.513</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.371</td>\n",
              "      <td>2.907</td>\n",
              "      <td>2.434</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.717</td>\n",
              "      <td>2.096</td>\n",
              "      <td>2.335</td>\n",
              "      <td>2.996</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.851</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.426</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.476</td>\n",
              "      <td>2.625</td>\n",
              "      <td>2.357</td>\n",
              "      <td>3.116</td>\n",
              "      <td>2.263</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.533</td>\n",
              "      <td>3.715</td>\n",
              "      <td>2.815</td>\n",
              "      <td></td>\n",
              "      <td>2.464</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.035</td>\n",
              "      <td>3.865</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.850</td>\n",
              "      <td>2.524</td>\n",
              "      <td>3.213</td>\n",
              "      <td>2.489</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.209</td>\n",
              "      <td>2.310</td>\n",
              "      <td>2.935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>4928</td>\n",
              "      <td>2</td>\n",
              "      <td>77.8</td>\n",
              "      <td>27</td>\n",
              "      <td>17.00</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.246</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.392</td>\n",
              "      <td>2.158</td>\n",
              "      <td>1.686</td>\n",
              "      <td>1.914</td>\n",
              "      <td>2.318</td>\n",
              "      <td>2.297</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.908</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.067</td>\n",
              "      <td>2.385</td>\n",
              "      <td>2.458</td>\n",
              "      <td>2.749</td>\n",
              "      <td>2.360</td>\n",
              "      <td></td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.919</td>\n",
              "      <td>2.063</td>\n",
              "      <td>2.426</td>\n",
              "      <td>2.284</td>\n",
              "      <td>1.906</td>\n",
              "      <td>3.070</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.235</td>\n",
              "      <td>2.685</td>\n",
              "      <td>2.419</td>\n",
              "      <td>1.953</td>\n",
              "      <td>2.660</td>\n",
              "      <td>1.815</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.349</td>\n",
              "      <td>2.211</td>\n",
              "      <td>2.330</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.196</td>\n",
              "      <td>1.565</td>\n",
              "      <td>1.845</td>\n",
              "      <td>2.281</td>\n",
              "      <td>2.255</td>\n",
              "      <td>2.266</td>\n",
              "      <td>2.893</td>\n",
              "      <td>2.167</td>\n",
              "      <td>2.420</td>\n",
              "      <td>1.934</td>\n",
              "      <td>2.468</td>\n",
              "      <td>2.420</td>\n",
              "      <td>3.288</td>\n",
              "      <td>2.411</td>\n",
              "      <td></td>\n",
              "      <td>2.147</td>\n",
              "      <td>2.478</td>\n",
              "      <td>2.319</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.106</td>\n",
              "      <td>2.742</td>\n",
              "      <td>2.369</td>\n",
              "      <td>2.221</td>\n",
              "      <td>2.485</td>\n",
              "      <td>2.162</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.375</td>\n",
              "      <td>1.917</td>\n",
              "      <td>2.199</td>\n",
              "      <td>2.709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>4887</td>\n",
              "      <td>3</td>\n",
              "      <td>73.8</td>\n",
              "      <td>26</td>\n",
              "      <td>27.00</td>\n",
              "      <td>1.996</td>\n",
              "      <td>2.545</td>\n",
              "      <td>2.196</td>\n",
              "      <td>2.263</td>\n",
              "      <td>2.099</td>\n",
              "      <td>1.343</td>\n",
              "      <td>1.858</td>\n",
              "      <td>2.676</td>\n",
              "      <td>2.135</td>\n",
              "      <td>1.955</td>\n",
              "      <td>2.882</td>\n",
              "      <td>2.042</td>\n",
              "      <td>2.448</td>\n",
              "      <td>1.874</td>\n",
              "      <td>2.486</td>\n",
              "      <td>2.061</td>\n",
              "      <td>3.193</td>\n",
              "      <td>1.800</td>\n",
              "      <td></td>\n",
              "      <td>3.054</td>\n",
              "      <td>2.921</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.629</td>\n",
              "      <td>2.119</td>\n",
              "      <td>1.730</td>\n",
              "      <td>3.190</td>\n",
              "      <td>2.839</td>\n",
              "      <td>2.443</td>\n",
              "      <td>1.948</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.567</td>\n",
              "      <td>1.854</td>\n",
              "      <td>2.713</td>\n",
              "      <td>1.646</td>\n",
              "      <td>2.698</td>\n",
              "      <td>2.424</td>\n",
              "      <td>1.969</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.231</td>\n",
              "      <td>2.447</td>\n",
              "      <td>2.340</td>\n",
              "      <td>1.472</td>\n",
              "      <td>1.775</td>\n",
              "      <td>2.832</td>\n",
              "      <td>2.014</td>\n",
              "      <td>2.200</td>\n",
              "      <td>2.720</td>\n",
              "      <td>2.320</td>\n",
              "      <td>2.427</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.362</td>\n",
              "      <td>2.122</td>\n",
              "      <td>3.244</td>\n",
              "      <td>2.270</td>\n",
              "      <td></td>\n",
              "      <td>2.577</td>\n",
              "      <td>2.239</td>\n",
              "      <td>2.084</td>\n",
              "      <td>1.682</td>\n",
              "      <td>2.797</td>\n",
              "      <td>3.351</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.585</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.683</td>\n",
              "      <td>1.701</td>\n",
              "      <td>2.749</td>\n",
              "      <td>2.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>1205</td>\n",
              "      <td>3</td>\n",
              "      <td>83.0</td>\n",
              "      <td>23</td>\n",
              "      <td>29.33</td>\n",
              "      <td>1.822</td>\n",
              "      <td>2.508</td>\n",
              "      <td>2.174</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.000</td>\n",
              "      <td>1.374</td>\n",
              "      <td>1.465</td>\n",
              "      <td>2.589</td>\n",
              "      <td>1.677</td>\n",
              "      <td>1.932</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.215</td>\n",
              "      <td>1.617</td>\n",
              "      <td>2.066</td>\n",
              "      <td>1.910</td>\n",
              "      <td>3.069</td>\n",
              "      <td>1.536</td>\n",
              "      <td>1.043</td>\n",
              "      <td>2.902</td>\n",
              "      <td>2.809</td>\n",
              "      <td>1.875</td>\n",
              "      <td>2.716</td>\n",
              "      <td>2.001</td>\n",
              "      <td>1.396</td>\n",
              "      <td>2.681</td>\n",
              "      <td>1.613</td>\n",
              "      <td>2.239</td>\n",
              "      <td>1.966</td>\n",
              "      <td>2.561</td>\n",
              "      <td>2.520</td>\n",
              "      <td>1.648</td>\n",
              "      <td>2.414</td>\n",
              "      <td>1.736</td>\n",
              "      <td>1.962</td>\n",
              "      <td>2.296</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.468</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.079</td>\n",
              "      <td>1.926</td>\n",
              "      <td>1.232</td>\n",
              "      <td>1.501</td>\n",
              "      <td>2.476</td>\n",
              "      <td>1.831</td>\n",
              "      <td>1.939</td>\n",
              "      <td>3.250</td>\n",
              "      <td>1.900</td>\n",
              "      <td>2.403</td>\n",
              "      <td>1.680</td>\n",
              "      <td>2.193</td>\n",
              "      <td>1.968</td>\n",
              "      <td>3.351</td>\n",
              "      <td>1.494</td>\n",
              "      <td>0.997</td>\n",
              "      <td>1.828</td>\n",
              "      <td>3.025</td>\n",
              "      <td>1.852</td>\n",
              "      <td>1.463</td>\n",
              "      <td>2.982</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.408</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.449</td>\n",
              "      <td>2.574</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.515</td>\n",
              "      <td>1.806</td>\n",
              "      <td>2.071</td>\n",
              "      <td>2.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>2099</td>\n",
              "      <td>2</td>\n",
              "      <td>83.7</td>\n",
              "      <td>30</td>\n",
              "      <td>12.00</td>\n",
              "      <td>2.245</td>\n",
              "      <td>2.351</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.475</td>\n",
              "      <td>2.155</td>\n",
              "      <td>1.676</td>\n",
              "      <td>1.857</td>\n",
              "      <td>2.525</td>\n",
              "      <td>2.085</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.161</td>\n",
              "      <td>2.295</td>\n",
              "      <td>1.948</td>\n",
              "      <td>2.424</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.958</td>\n",
              "      <td>1.778</td>\n",
              "      <td></td>\n",
              "      <td>2.672</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.693</td>\n",
              "      <td>2.315</td>\n",
              "      <td>1.637</td>\n",
              "      <td>3.445</td>\n",
              "      <td>2.396</td>\n",
              "      <td>2.432</td>\n",
              "      <td>2.265</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.021</td>\n",
              "      <td>2.350</td>\n",
              "      <td>1.699</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.625</td>\n",
              "      <td>2.034</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.237</td>\n",
              "      <td>2.236</td>\n",
              "      <td>2.314</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.868</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.072</td>\n",
              "      <td>2.060</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.155</td>\n",
              "      <td>2.376</td>\n",
              "      <td>1.894</td>\n",
              "      <td>2.400</td>\n",
              "      <td>2.330</td>\n",
              "      <td>3.177</td>\n",
              "      <td>2.054</td>\n",
              "      <td></td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.433</td>\n",
              "      <td>2.206</td>\n",
              "      <td>1.785</td>\n",
              "      <td>3.093</td>\n",
              "      <td>2.310</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.381</td>\n",
              "      <td>1.817</td>\n",
              "      <td>2.121</td>\n",
              "      <td>2.586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1363 rows × 75 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       RID  ...  ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
              "0     4084  ...                                       2.862\n",
              "1     2196  ...                                       2.943\n",
              "2      657  ...                                       2.828\n",
              "3     4526  ...                                       2.591\n",
              "4      362  ...                                       2.245\n",
              "...    ...  ...                                         ...\n",
              "1358  4187  ...                                       2.935\n",
              "1359  4928  ...                                       2.709\n",
              "1360  4887  ...                                       2.777\n",
              "1361  1205  ...                                       2.248\n",
              "1362  2099  ...                                       2.586\n",
              "\n",
              "[1363 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3aPdHTByT8B"
      },
      "source": [
        "#WRITE YOUR CODE HERE!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuFRiRUyAFL",
        "outputId": "6606ed7d-85ff-4b8c-ff46-766b7dca9ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# the result of the midterm project\n",
        "for i in train_data.columns:#since there are object type data and null, it needs to be changed.\n",
        "  if train_data[i].dtype=='object':#for object type data column.\n",
        "    train_data[i]=pd.to_numeric(train_data[i], errors='coerce')#numeric entries are changed to float type and missing entries are replaced with NaN.\n",
        "  if train_data[i].isnull().any():#missing entries are replaced with NaN.\n",
        "    train_data[i].loc[train_data[i].isnull()]=np.NaN\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "train_data= SimpleImputer(strategy='median').fit_transform(train_data)#replace missing values using the median along each column.\n",
        "train_columns_list=[i for i in range(len(train_data[0])) if i not in [0,1,3,4]]#a columns list for training data(without RID, DXCHANGE, MMSE, ADAS13)\n",
        "x=StandardScaler().fit_transform(train_data[:, train_columns_list])#an ndarray for training data #index 699 row of the initial data order has some median data.\n",
        "y=train_data[:, [3, 4]]# an ndarray for target data(MMSE, ADAS13)\n",
        "dxchange=train_data[:,1]# an array for target data(DXCHANGE)\n",
        "\n",
        "#regrerssion task\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "lin_reg=LinearRegression()#making linear regression model\n",
        "lin_reg.fit(x,y)#fitting train data and target\n",
        "#10 fold cross validation score\n",
        "print('mean square error: ',np.sqrt(-cross_val_score(lin_reg, x, y, scoring=\"neg_mean_squared_error\", cv=10)).mean())#since the test data is 20% of the total data, I chose k=10.\n",
        "\n",
        "#classification task. since it took a lot of time, I just implemented the result of the grid search.\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_clf=SGDClassifier(alpha=0.15599452033620265, average=False, class_weight=None,\n",
        "              early_stopping=True, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
        "              l1_ratio=0.020584494295802447, learning_rate='adaptive',\n",
        "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
        "              penalty='elasticnet', power_t=0.5, random_state=None,\n",
        "              shuffle=True, tol=0.001, validation_fraction=0.2, verbose=0,\n",
        "              warm_start=False)\n",
        "sgd_clf.fit(x,dxchange)#Avg. loss: 0.274994\n",
        "print('score: ',sgd_clf.score(x, dxchange))#since the test data is 20% of the total data, I chose k=10."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean square error:  5.27733401244674\n",
            "score:  0.5656639765223771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTgpljyD79Z",
        "outputId": "ebef21c1-1697-49c5-bc17-b7aec2a10229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# the code for this assignment\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQPpI-_SMKpt"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#assert tf.__version__ >= \"2.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6cEDqrCRQH7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92dc1cae-bb91-4662-cbee-8cb571ca279c"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_mmse=y[:,0]#mmse data\n",
        "y_adas13=y[:,1]#adas13 data\n",
        "y_dxchange=to_categorical(dxchange)#making dxchange as on-hot vector\n",
        "y_all=train_data[:,[1,3,4]]#dxchange, mmse, adas13 data\n",
        "# these are already imputed and scaled.\n",
        "#since there are no best way to choose hyperparameter, I just randomly tried and the below one was the best.\n",
        "#I followed the recommendation of the lecture note 6\n",
        "input_one=keras.layers.Input(shape=[71,])\n",
        "hidden1=keras.layers.Dense(60, activation=\"selu\", kernel_initializer=\"lecun_normal\")(input_one)\n",
        "hidden2=keras.layers.Dense(50, activation=\"selu\", kernel_initializer=\"lecun_normal\")(hidden1)\n",
        "\n",
        "concat=keras.layers.concatenate([input_one, hidden2])#for wider neural network\n",
        "\n",
        "mmse=keras.layers.Dense(1, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"MMSE\")(concat)#MMSE output\n",
        "adas13=keras.layers.Dense(1, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"ADAS13\")(concat)#ADAS13 output\n",
        "DC=keras.layers.Dense(4, activation=\"softmax\", kernel_initializer=\"glorot_uniform\", name=\"DXCHANGE\")(concat)#dxchange ouput\n",
        "aux_output1=keras.layers.Dense(2)(hidden2)#auxiliary output for the regularization technique\n",
        "\n",
        "model=keras.models.Model(inputs=[input_one], outputs=[mmse, adas13, DC, aux_output1])#the neural network\n",
        "model.compile(loss=[\"mse\", \"mse\", \"categorical_crossentropy\", \"mse\"], loss_weights=[1, 1, 1, 0.1],\n",
        "              optimizer=\"nadam\", metrics={\"MMSE\": \"mse\", \"ADAS13\": \"mse\", \"DXCHANGE\": 'accuracy'})\n",
        "es=keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)#early stopping callback\n",
        "history=model.fit(#as I have done in mid term project, I used 0.2 validation data\n",
        "    (x), (y_mmse, y_adas13, to_categorical(dxchange), y), epochs=1000,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[es]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1090 samples, validate on 273 samples\n",
            "Epoch 1/1000\n",
            "1090/1090 [==============================] - 14s 13ms/sample - loss: 1053.7990 - MMSE_loss: 682.3038 - ADAS13_loss: 314.4277 - DXCHANGE_loss: 1.8053 - dense_363_loss: 558.6465 - MMSE_mean_squared_error: 675.8224 - ADAS13_mean_squared_error: 320.1769 - DXCHANGE_acc: 0.2404 - val_loss: 854.6492 - val_MMSE_loss: 558.3250 - val_ADAS13_loss: 240.6903 - val_DXCHANGE_loss: 1.4035 - val_dense_363_loss: 543.1323 - val_MMSE_mean_squared_error: 556.1771 - val_ADAS13_mean_squared_error: 242.6576 - val_DXCHANGE_acc: 0.3919\n",
            "Epoch 2/1000\n",
            "1090/1090 [==============================] - 1s 571us/sample - loss: 738.9459 - MMSE_loss: 467.5975 - ADAS13_loss: 223.8336 - DXCHANGE_loss: 1.2309 - dense_363_loss: 449.1541 - MMSE_mean_squared_error: 467.8206 - ADAS13_mean_squared_error: 224.3087 - DXCHANGE_acc: 0.4202 - val_loss: 694.3781 - val_MMSE_loss: 444.1051 - val_ADAS13_loss: 217.5838 - val_DXCHANGE_loss: 1.3295 - val_dense_363_loss: 336.0833 - val_MMSE_mean_squared_error: 441.5485 - val_ADAS13_mean_squared_error: 218.0750 - val_DXCHANGE_acc: 0.4359\n",
            "Epoch 3/1000\n",
            "1090/1090 [==============================] - 1s 586us/sample - loss: 666.1453 - MMSE_loss: 423.6440 - ADAS13_loss: 215.1960 - DXCHANGE_loss: 1.1506 - dense_363_loss: 266.9224 - MMSE_mean_squared_error: 426.7958 - ADAS13_mean_squared_error: 211.3244 - DXCHANGE_acc: 0.4514 - val_loss: 661.1818 - val_MMSE_loss: 431.4982 - val_ADAS13_loss: 212.5400 - val_DXCHANGE_loss: 1.2200 - val_dense_363_loss: 241.5486 - val_MMSE_mean_squared_error: 425.2943 - val_ADAS13_mean_squared_error: 210.6463 - val_DXCHANGE_acc: 0.4286\n",
            "Epoch 4/1000\n",
            "1090/1090 [==============================] - 1s 585us/sample - loss: 635.6396 - MMSE_loss: 412.9323 - ADAS13_loss: 202.1227 - DXCHANGE_loss: 1.0673 - dense_363_loss: 206.2924 - MMSE_mean_squared_error: 411.2676 - ADAS13_mean_squared_error: 202.6266 - DXCHANGE_acc: 0.4596 - val_loss: 639.0921 - val_MMSE_loss: 399.3384 - val_ADAS13_loss: 206.9227 - val_DXCHANGE_loss: 1.2983 - val_dense_363_loss: 206.4837 - val_MMSE_mean_squared_error: 411.3446 - val_ADAS13_mean_squared_error: 205.4731 - val_DXCHANGE_acc: 0.4505\n",
            "Epoch 5/1000\n",
            "1090/1090 [==============================] - 1s 562us/sample - loss: 614.4809 - MMSE_loss: 399.5213 - ADAS13_loss: 196.6600 - DXCHANGE_loss: 1.0685 - dense_363_loss: 183.4069 - MMSE_mean_squared_error: 398.4338 - ADAS13_mean_squared_error: 196.6286 - DXCHANGE_acc: 0.4881 - val_loss: 616.5878 - val_MMSE_loss: 394.9893 - val_ADAS13_loss: 200.2180 - val_DXCHANGE_loss: 1.0407 - val_dense_363_loss: 193.8603 - val_MMSE_mean_squared_error: 398.2588 - val_ADAS13_mean_squared_error: 197.8981 - val_DXCHANGE_acc: 0.4835\n",
            "Epoch 6/1000\n",
            "1090/1090 [==============================] - 1s 557us/sample - loss: 593.6675 - MMSE_loss: 385.0789 - ADAS13_loss: 194.3343 - DXCHANGE_loss: 1.0186 - dense_363_loss: 168.3269 - MMSE_mean_squared_error: 385.6365 - ADAS13_mean_squared_error: 190.3088 - DXCHANGE_acc: 0.4991 - val_loss: 599.1836 - val_MMSE_loss: 386.8919 - val_ADAS13_loss: 195.3152 - val_DXCHANGE_loss: 1.1883 - val_dense_363_loss: 179.0463 - val_MMSE_mean_squared_error: 384.1053 - val_ADAS13_mean_squared_error: 195.7938 - val_DXCHANGE_acc: 0.4542\n",
            "Epoch 7/1000\n",
            "1090/1090 [==============================] - 1s 581us/sample - loss: 575.9051 - MMSE_loss: 372.8934 - ADAS13_loss: 187.1695 - DXCHANGE_loss: 0.9850 - dense_363_loss: 151.2422 - MMSE_mean_squared_error: 373.0995 - ADAS13_mean_squared_error: 186.3854 - DXCHANGE_acc: 0.5202 - val_loss: 578.6604 - val_MMSE_loss: 378.8125 - val_ADAS13_loss: 187.0119 - val_DXCHANGE_loss: 1.0464 - val_dense_363_loss: 170.0960 - val_MMSE_mean_squared_error: 371.9773 - val_ADAS13_mean_squared_error: 188.6962 - val_DXCHANGE_acc: 0.4689\n",
            "Epoch 8/1000\n",
            "1090/1090 [==============================] - 1s 577us/sample - loss: 558.6287 - MMSE_loss: 360.3247 - ADAS13_loss: 178.6191 - DXCHANGE_loss: 0.9565 - dense_363_loss: 139.1108 - MMSE_mean_squared_error: 361.1022 - ADAS13_mean_squared_error: 182.5675 - DXCHANGE_acc: 0.5275 - val_loss: 564.7342 - val_MMSE_loss: 362.2923 - val_ADAS13_loss: 188.9120 - val_DXCHANGE_loss: 0.9633 - val_dense_363_loss: 156.1107 - val_MMSE_mean_squared_error: 362.5579 - val_ADAS13_mean_squared_error: 185.7456 - val_DXCHANGE_acc: 0.5018\n",
            "Epoch 9/1000\n",
            "1090/1090 [==============================] - 1s 590us/sample - loss: 545.0422 - MMSE_loss: 365.2848 - ADAS13_loss: 175.4817 - DXCHANGE_loss: 0.9246 - dense_363_loss: 127.7024 - MMSE_mean_squared_error: 351.7159 - ADAS13_mean_squared_error: 179.8676 - DXCHANGE_acc: 0.5450 - val_loss: 557.6693 - val_MMSE_loss: 363.9467 - val_ADAS13_loss: 180.9815 - val_DXCHANGE_loss: 0.9615 - val_dense_363_loss: 142.8330 - val_MMSE_mean_squared_error: 356.3775 - val_ADAS13_mean_squared_error: 186.0221 - val_DXCHANGE_acc: 0.4982\n",
            "Epoch 10/1000\n",
            "1090/1090 [==============================] - 1s 606us/sample - loss: 534.7990 - MMSE_loss: 346.7042 - ADAS13_loss: 173.5565 - DXCHANGE_loss: 0.9141 - dense_363_loss: 111.7818 - MMSE_mean_squared_error: 344.8512 - ADAS13_mean_squared_error: 177.8743 - DXCHANGE_acc: 0.5523 - val_loss: 548.7260 - val_MMSE_loss: 347.3372 - val_ADAS13_loss: 183.5075 - val_DXCHANGE_loss: 0.9998 - val_dense_363_loss: 128.9994 - val_MMSE_mean_squared_error: 351.4153 - val_ADAS13_mean_squared_error: 183.3969 - val_DXCHANGE_acc: 0.5092\n",
            "Epoch 11/1000\n",
            "1090/1090 [==============================] - 1s 630us/sample - loss: 525.1565 - MMSE_loss: 329.9168 - ADAS13_loss: 172.6552 - DXCHANGE_loss: 0.9109 - dense_363_loss: 98.0967 - MMSE_mean_squared_error: 338.8842 - ADAS13_mean_squared_error: 175.3582 - DXCHANGE_acc: 0.5450 - val_loss: 538.4065 - val_MMSE_loss: 333.4073 - val_ADAS13_loss: 181.4547 - val_DXCHANGE_loss: 1.0918 - val_dense_363_loss: 115.7004 - val_MMSE_mean_squared_error: 343.3228 - val_ADAS13_mean_squared_error: 182.2343 - val_DXCHANGE_acc: 0.4542\n",
            "Epoch 12/1000\n",
            "1090/1090 [==============================] - 1s 630us/sample - loss: 514.2057 - MMSE_loss: 322.1980 - ADAS13_loss: 169.6141 - DXCHANGE_loss: 0.8916 - dense_363_loss: 87.8536 - MMSE_mean_squared_error: 330.7663 - ADAS13_mean_squared_error: 173.6318 - DXCHANGE_acc: 0.5468 - val_loss: 527.4540 - val_MMSE_loss: 331.1061 - val_ADAS13_loss: 181.1702 - val_DXCHANGE_loss: 0.9157 - val_dense_363_loss: 104.1277 - val_MMSE_mean_squared_error: 333.3229 - val_ADAS13_mean_squared_error: 182.6527 - val_DXCHANGE_acc: 0.5275\n",
            "Epoch 13/1000\n",
            "1090/1090 [==============================] - 1s 630us/sample - loss: 501.0381 - MMSE_loss: 325.9570 - ADAS13_loss: 173.9291 - DXCHANGE_loss: 0.8795 - dense_363_loss: 83.1890 - MMSE_mean_squared_error: 320.7905 - ADAS13_mean_squared_error: 171.4478 - DXCHANGE_acc: 0.5697 - val_loss: 506.8510 - val_MMSE_loss: 310.0150 - val_ADAS13_loss: 184.5855 - val_DXCHANGE_loss: 0.9251 - val_dense_363_loss: 92.5596 - val_MMSE_mean_squared_error: 313.0906 - val_ADAS13_mean_squared_error: 183.5461 - val_DXCHANGE_acc: 0.5311\n",
            "Epoch 14/1000\n",
            "1090/1090 [==============================] - 1s 605us/sample - loss: 476.2407 - MMSE_loss: 303.7043 - ADAS13_loss: 166.4251 - DXCHANGE_loss: 0.8452 - dense_363_loss: 69.3157 - MMSE_mean_squared_error: 299.1427 - ADAS13_mean_squared_error: 169.4187 - DXCHANGE_acc: 0.5688 - val_loss: 458.3872 - val_MMSE_loss: 269.0715 - val_ADAS13_loss: 179.8870 - val_DXCHANGE_loss: 0.9258 - val_dense_363_loss: 77.5515 - val_MMSE_mean_squared_error: 270.2158 - val_ADAS13_mean_squared_error: 179.6412 - val_DXCHANGE_acc: 0.5421\n",
            "Epoch 15/1000\n",
            "1090/1090 [==============================] - 1s 570us/sample - loss: 334.7366 - MMSE_loss: 162.6753 - ADAS13_loss: 162.4307 - DXCHANGE_loss: 0.8609 - dense_363_loss: 62.7209 - MMSE_mean_squared_error: 163.4337 - ADAS13_mean_squared_error: 164.4915 - DXCHANGE_acc: 0.5596 - val_loss: 229.9852 - val_MMSE_loss: 56.1951 - val_ADAS13_loss: 166.1565 - val_DXCHANGE_loss: 0.9763 - val_dense_363_loss: 64.8626 - val_MMSE_mean_squared_error: 53.7927 - val_ADAS13_mean_squared_error: 168.7451 - val_DXCHANGE_acc: 0.5348\n",
            "Epoch 16/1000\n",
            "1090/1090 [==============================] - 1s 627us/sample - loss: 174.6884 - MMSE_loss: 22.5623 - ADAS13_loss: 145.3428 - DXCHANGE_loss: 0.8624 - dense_363_loss: 47.0299 - MMSE_mean_squared_error: 22.7228 - ADAS13_mean_squared_error: 146.2749 - DXCHANGE_acc: 0.5615 - val_loss: 163.8305 - val_MMSE_loss: 20.8654 - val_ADAS13_loss: 139.0649 - val_DXCHANGE_loss: 0.9612 - val_dense_363_loss: 48.9765 - val_MMSE_mean_squared_error: 20.1116 - val_ADAS13_mean_squared_error: 137.8229 - val_DXCHANGE_acc: 0.5421\n",
            "Epoch 17/1000\n",
            "1090/1090 [==============================] - 1s 600us/sample - loss: 96.0352 - MMSE_loss: 13.8140 - ADAS13_loss: 75.3511 - DXCHANGE_loss: 0.8699 - dense_363_loss: 42.5621 - MMSE_mean_squared_error: 13.9566 - ADAS13_mean_squared_error: 76.8982 - DXCHANGE_acc: 0.5716 - val_loss: 77.8980 - val_MMSE_loss: 15.5269 - val_ADAS13_loss: 58.0019 - val_DXCHANGE_loss: 1.0193 - val_dense_363_loss: 46.7100 - val_MMSE_mean_squared_error: 15.0760 - val_ADAS13_mean_squared_error: 57.3076 - val_DXCHANGE_acc: 0.4982\n",
            "Epoch 18/1000\n",
            "1090/1090 [==============================] - 1s 601us/sample - loss: 66.1012 - MMSE_loss: 10.0945 - ADAS13_loss: 49.9518 - DXCHANGE_loss: 0.8599 - dense_363_loss: 36.8712 - MMSE_mean_squared_error: 10.1833 - ADAS13_mean_squared_error: 51.3053 - DXCHANGE_acc: 0.5853 - val_loss: 67.3681 - val_MMSE_loss: 10.6266 - val_ADAS13_loss: 51.7329 - val_DXCHANGE_loss: 0.9631 - val_dense_363_loss: 39.3617 - val_MMSE_mean_squared_error: 10.5881 - val_ADAS13_mean_squared_error: 51.9459 - val_DXCHANGE_acc: 0.5348\n",
            "Epoch 19/1000\n",
            "1090/1090 [==============================] - 1s 586us/sample - loss: 60.6458 - MMSE_loss: 8.4314 - ADAS13_loss: 47.6816 - DXCHANGE_loss: 0.8713 - dense_363_loss: 34.5461 - MMSE_mean_squared_error: 8.2236 - ADAS13_mean_squared_error: 48.1416 - DXCHANGE_acc: 0.5881 - val_loss: 63.4396 - val_MMSE_loss: 8.6779 - val_ADAS13_loss: 48.8486 - val_DXCHANGE_loss: 0.9245 - val_dense_363_loss: 35.8724 - val_MMSE_mean_squared_error: 8.8470 - val_ADAS13_mean_squared_error: 50.0443 - val_DXCHANGE_acc: 0.5531\n",
            "Epoch 20/1000\n",
            "1090/1090 [==============================] - 1s 575us/sample - loss: 57.6599 - MMSE_loss: 7.1936 - ADAS13_loss: 44.9616 - DXCHANGE_loss: 0.8347 - dense_363_loss: 31.7816 - MMSE_mean_squared_error: 7.3676 - ADAS13_mean_squared_error: 46.1975 - DXCHANGE_acc: 0.5862 - val_loss: 62.1475 - val_MMSE_loss: 8.3646 - val_ADAS13_loss: 48.8107 - val_DXCHANGE_loss: 1.0011 - val_dense_363_loss: 34.8197 - val_MMSE_mean_squared_error: 8.2962 - val_ADAS13_mean_squared_error: 49.3337 - val_DXCHANGE_acc: 0.4982\n",
            "Epoch 21/1000\n",
            "1090/1090 [==============================] - 1s 560us/sample - loss: 55.8720 - MMSE_loss: 6.7976 - ADAS13_loss: 45.1734 - DXCHANGE_loss: 0.8251 - dense_363_loss: 31.3281 - MMSE_mean_squared_error: 6.8670 - ADAS13_mean_squared_error: 45.0664 - DXCHANGE_acc: 0.5881 - val_loss: 61.1909 - val_MMSE_loss: 7.1461 - val_ADAS13_loss: 49.5018 - val_DXCHANGE_loss: 0.9108 - val_dense_363_loss: 34.1384 - val_MMSE_mean_squared_error: 7.1641 - val_ADAS13_mean_squared_error: 49.6616 - val_DXCHANGE_acc: 0.5714\n",
            "Epoch 22/1000\n",
            "1090/1090 [==============================] - 1s 561us/sample - loss: 54.6988 - MMSE_loss: 6.3347 - ADAS13_loss: 44.1256 - DXCHANGE_loss: 0.8013 - dense_363_loss: 29.9730 - MMSE_mean_squared_error: 6.5078 - ADAS13_mean_squared_error: 44.3770 - DXCHANGE_acc: 0.6055 - val_loss: 64.5191 - val_MMSE_loss: 8.7683 - val_ADAS13_loss: 51.7904 - val_DXCHANGE_loss: 0.9288 - val_dense_363_loss: 36.1862 - val_MMSE_mean_squared_error: 8.6716 - val_ADAS13_mean_squared_error: 51.2807 - val_DXCHANGE_acc: 0.5604\n",
            "Epoch 23/1000\n",
            "1090/1090 [==============================] - 1s 593us/sample - loss: 53.8015 - MMSE_loss: 5.9998 - ADAS13_loss: 43.5453 - DXCHANGE_loss: 0.8122 - dense_363_loss: 28.9657 - MMSE_mean_squared_error: 6.0859 - ADAS13_mean_squared_error: 43.9842 - DXCHANGE_acc: 0.6092 - val_loss: 59.4069 - val_MMSE_loss: 6.9059 - val_ADAS13_loss: 46.9074 - val_DXCHANGE_loss: 0.9550 - val_dense_363_loss: 31.2220 - val_MMSE_mean_squared_error: 7.0650 - val_ADAS13_mean_squared_error: 48.1996 - val_DXCHANGE_acc: 0.5238\n",
            "Epoch 24/1000\n",
            "1090/1090 [==============================] - 1s 550us/sample - loss: 52.4308 - MMSE_loss: 6.0761 - ADAS13_loss: 42.5118 - DXCHANGE_loss: 0.8009 - dense_363_loss: 28.0345 - MMSE_mean_squared_error: 6.0285 - ADAS13_mean_squared_error: 42.7938 - DXCHANGE_acc: 0.6183 - val_loss: 62.1847 - val_MMSE_loss: 6.8818 - val_ADAS13_loss: 52.1800 - val_DXCHANGE_loss: 0.9250 - val_dense_363_loss: 34.4923 - val_MMSE_mean_squared_error: 6.8980 - val_ADAS13_mean_squared_error: 50.9452 - val_DXCHANGE_acc: 0.5421\n",
            "Epoch 25/1000\n",
            "1090/1090 [==============================] - 1s 547us/sample - loss: 52.0337 - MMSE_loss: 5.8588 - ADAS13_loss: 46.2134 - DXCHANGE_loss: 0.8076 - dense_363_loss: 29.4648 - MMSE_mean_squared_error: 5.9119 - ADAS13_mean_squared_error: 42.5533 - DXCHANGE_acc: 0.6239 - val_loss: 59.6327 - val_MMSE_loss: 6.8774 - val_ADAS13_loss: 47.5435 - val_DXCHANGE_loss: 0.8978 - val_dense_363_loss: 30.7265 - val_MMSE_mean_squared_error: 6.9320 - val_ADAS13_mean_squared_error: 48.6624 - val_DXCHANGE_acc: 0.5678\n",
            "Epoch 26/1000\n",
            "1090/1090 [==============================] - 1s 561us/sample - loss: 51.4179 - MMSE_loss: 5.7016 - ADAS13_loss: 42.1189 - DXCHANGE_loss: 0.7928 - dense_363_loss: 26.8244 - MMSE_mean_squared_error: 5.7331 - ADAS13_mean_squared_error: 42.2075 - DXCHANGE_acc: 0.6220 - val_loss: 58.1260 - val_MMSE_loss: 6.1642 - val_ADAS13_loss: 47.1021 - val_DXCHANGE_loss: 0.9568 - val_dense_363_loss: 29.8167 - val_MMSE_mean_squared_error: 6.2205 - val_ADAS13_mean_squared_error: 47.9177 - val_DXCHANGE_acc: 0.5092\n",
            "Epoch 27/1000\n",
            "1090/1090 [==============================] - 1s 549us/sample - loss: 50.5738 - MMSE_loss: 5.6027 - ADAS13_loss: 40.7069 - DXCHANGE_loss: 0.7979 - dense_363_loss: 25.5880 - MMSE_mean_squared_error: 5.5606 - ADAS13_mean_squared_error: 41.5966 - DXCHANGE_acc: 0.6257 - val_loss: 57.7387 - val_MMSE_loss: 5.8720 - val_ADAS13_loss: 46.7191 - val_DXCHANGE_loss: 0.9607 - val_dense_363_loss: 29.3185 - val_MMSE_mean_squared_error: 5.8932 - val_ADAS13_mean_squared_error: 47.8902 - val_DXCHANGE_acc: 0.5238\n",
            "Epoch 28/1000\n",
            "1090/1090 [==============================] - 1s 536us/sample - loss: 50.1632 - MMSE_loss: 5.2747 - ADAS13_loss: 40.9021 - DXCHANGE_loss: 0.7806 - dense_363_loss: 25.4257 - MMSE_mean_squared_error: 5.3693 - ADAS13_mean_squared_error: 41.4210 - DXCHANGE_acc: 0.6183 - val_loss: 58.4482 - val_MMSE_loss: 6.0970 - val_ADAS13_loss: 47.3596 - val_DXCHANGE_loss: 0.8913 - val_dense_363_loss: 29.4785 - val_MMSE_mean_squared_error: 6.2395 - val_ADAS13_mean_squared_error: 48.2991 - val_DXCHANGE_acc: 0.5421\n",
            "Epoch 29/1000\n",
            "1090/1090 [==============================] - 1s 552us/sample - loss: 49.4776 - MMSE_loss: 5.1994 - ADAS13_loss: 40.0475 - DXCHANGE_loss: 0.7755 - dense_363_loss: 24.9908 - MMSE_mean_squared_error: 5.3211 - ADAS13_mean_squared_error: 40.8258 - DXCHANGE_acc: 0.6312 - val_loss: 58.7474 - val_MMSE_loss: 6.5116 - val_ADAS13_loss: 49.9936 - val_DXCHANGE_loss: 0.9339 - val_dense_363_loss: 31.1904 - val_MMSE_mean_squared_error: 6.6069 - val_ADAS13_mean_squared_error: 48.1654 - val_DXCHANGE_acc: 0.5201\n",
            "Epoch 30/1000\n",
            "1090/1090 [==============================] - 1s 567us/sample - loss: 49.0762 - MMSE_loss: 5.1234 - ADAS13_loss: 40.4126 - DXCHANGE_loss: 0.7805 - dense_363_loss: 24.8863 - MMSE_mean_squared_error: 5.2155 - ADAS13_mean_squared_error: 40.5708 - DXCHANGE_acc: 0.6303 - val_loss: 63.2625 - val_MMSE_loss: 6.1347 - val_ADAS13_loss: 52.1043 - val_DXCHANGE_loss: 0.9107 - val_dense_363_loss: 32.5172 - val_MMSE_mean_squared_error: 6.1581 - val_ADAS13_mean_squared_error: 52.8902 - val_DXCHANGE_acc: 0.5531\n",
            "Epoch 31/1000\n",
            "1090/1090 [==============================] - 1s 567us/sample - loss: 48.8381 - MMSE_loss: 5.2711 - ADAS13_loss: 39.3511 - DXCHANGE_loss: 0.7664 - dense_363_loss: 24.5535 - MMSE_mean_squared_error: 5.2381 - ADAS13_mean_squared_error: 40.3295 - DXCHANGE_acc: 0.6349 - val_loss: 57.5393 - val_MMSE_loss: 5.9034 - val_ADAS13_loss: 46.8295 - val_DXCHANGE_loss: 0.8906 - val_dense_363_loss: 28.9519 - val_MMSE_mean_squared_error: 5.9523 - val_ADAS13_mean_squared_error: 47.7534 - val_DXCHANGE_acc: 0.5568\n",
            "Epoch 32/1000\n",
            "1090/1090 [==============================] - 1s 531us/sample - loss: 47.8179 - MMSE_loss: 5.1548 - ADAS13_loss: 38.7475 - DXCHANGE_loss: 0.7717 - dense_363_loss: 23.9456 - MMSE_mean_squared_error: 5.1098 - ADAS13_mean_squared_error: 39.5013 - DXCHANGE_acc: 0.6367 - val_loss: 58.6911 - val_MMSE_loss: 6.6960 - val_ADAS13_loss: 51.3457 - val_DXCHANGE_loss: 0.8940 - val_dense_363_loss: 31.9842 - val_MMSE_mean_squared_error: 6.4406 - val_ADAS13_mean_squared_error: 48.3562 - val_DXCHANGE_acc: 0.5531\n",
            "Epoch 33/1000\n",
            "1090/1090 [==============================] - 1s 548us/sample - loss: 47.6158 - MMSE_loss: 5.0733 - ADAS13_loss: 43.4007 - DXCHANGE_loss: 0.8095 - dense_363_loss: 26.3466 - MMSE_mean_squared_error: 5.0702 - ADAS13_mean_squared_error: 39.3582 - DXCHANGE_acc: 0.6248 - val_loss: 60.4781 - val_MMSE_loss: 6.2130 - val_ADAS13_loss: 50.2706 - val_DXCHANGE_loss: 0.9180 - val_dense_363_loss: 30.8923 - val_MMSE_mean_squared_error: 6.1526 - val_ADAS13_mean_squared_error: 50.3128 - val_DXCHANGE_acc: 0.5238\n",
            "Epoch 34/1000\n",
            "1090/1090 [==============================] - 1s 573us/sample - loss: 47.7055 - MMSE_loss: 4.9279 - ADAS13_loss: 38.5508 - DXCHANGE_loss: 0.7791 - dense_363_loss: 23.7085 - MMSE_mean_squared_error: 5.0453 - ADAS13_mean_squared_error: 39.4612 - DXCHANGE_acc: 0.6339 - val_loss: 59.3138 - val_MMSE_loss: 6.0033 - val_ADAS13_loss: 49.8137 - val_DXCHANGE_loss: 0.8989 - val_dense_363_loss: 30.3836 - val_MMSE_mean_squared_error: 6.0181 - val_ADAS13_mean_squared_error: 49.3912 - val_DXCHANGE_acc: 0.5495\n",
            "Epoch 35/1000\n",
            "1090/1090 [==============================] - 1s 569us/sample - loss: 46.8086 - MMSE_loss: 4.8350 - ADAS13_loss: 37.9553 - DXCHANGE_loss: 0.7551 - dense_363_loss: 23.3408 - MMSE_mean_squared_error: 4.9216 - ADAS13_mean_squared_error: 38.7457 - DXCHANGE_acc: 0.6275 - val_loss: 60.5500 - val_MMSE_loss: 6.7464 - val_ADAS13_loss: 48.6129 - val_DXCHANGE_loss: 0.8943 - val_dense_363_loss: 29.6950 - val_MMSE_mean_squared_error: 6.8096 - val_ADAS13_mean_squared_error: 49.8025 - val_DXCHANGE_acc: 0.5531\n",
            "Epoch 36/1000\n",
            "1090/1090 [==============================] - 8s 7ms/sample - loss: 46.7080 - MMSE_loss: 5.0273 - ADAS13_loss: 37.5856 - DXCHANGE_loss: 0.7728 - dense_363_loss: 23.2758 - MMSE_mean_squared_error: 5.0708 - ADAS13_mean_squared_error: 38.5060 - DXCHANGE_acc: 0.6321 - val_loss: 60.3074 - val_MMSE_loss: 6.6145 - val_ADAS13_loss: 49.4987 - val_DXCHANGE_loss: 0.9263 - val_dense_363_loss: 30.2531 - val_MMSE_mean_squared_error: 6.6329 - val_ADAS13_mean_squared_error: 49.7265 - val_DXCHANGE_acc: 0.5604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwe8H3ZGuOHq",
        "cellView": "both"
      },
      "source": [
        "#I have not found the method for evaluating cross validation score for funcrional API model\n",
        "#So, for the comparing, I used mean squared error and accuracy.\n",
        "#However, since I made the result of regression task not separately before, I could not compare with the result of this assignment.\n",
        "#Therefore, let me just explain about the result.\n",
        "#MMSE and DXCHANGE results are seems to be similar to the mid-term project but, ADAS13 result is not really good."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}